# 第六章 概率与分布  

概率，从广义上讲，涉及不确定性的研究。概率可以被视为事件发生次数的比例，或者是我们对事件的信念程度。然后，我们希望使用这个概率来衡量实验中某事发生的可能性。正如在第1章中提到的，我们经常量化数据中的不确定性、机器学习模型中的不确定性以及模型产生的预测中的不确定性。量化不确定性需要随机变量的概念，随机变量是一个函数，它将随机实验的结果映射到我们感兴趣的属性集上。与随机变量相关联的是一个函数，用于测量特定结果（或一组结果）发生的概率；这被称为概率分布。  

随机变量  

概率分布  

概率分布被用作其他概念（如概率建模、图形模型和模型选择）的基础。在下一节中，我们将介绍定义概率空间的三个概念（样本空间、事件和事件的概率）以及它们与称为随机变量的第四个概念之间的关系。由于严谨的介绍可能会掩盖概念背后的直觉，因此本节的介绍故意略去了细节。本章介绍的概念概览如图6.1所示。



## 6.1 构建概率空间  

概率理论旨在定义一个数学结构来描述实验的随机结果。例如，当我们抛掷一枚硬币时，无法确定结果，但通过大量抛掷硬币，我们可以观察到平均结果的规律性。使用这种概率的数学结构，目标是实现自动化推理，从这个意义上讲，概率扩展了逻辑推理（Jaynes, 2003）。  

### 6.1.1 哲学问题  

在构建自动化推理系统时，经典布尔逻辑无法表达某些形式的合理推理。考虑以下场景：我们观察到   $A$  是假的。我们发现   $B$  变得不太可能，尽管无法从经典逻辑中得出结论。我们观察到   $B$  是真的。似乎   $A$  变得更有可能。我们每天都在使用这种推理方式。我们正在等待一个朋友，并考虑三种可能性：H1，她准时到达；H2，她因交通堵塞而迟到；H3，她被外星人绑架。当我们观察到朋友迟到时，我们必须逻辑地排除H1。我们倾向于认为H2更有可能，尽管我们没有逻辑上要求这样做。最后，我们可能会认为H3是可能的，但我们继续认为它相当不可能。我们如何得出H2是最有可能的答案？从这个角度来看，概率论可以被视为布尔逻辑的扩展。在机器学习的背景下，它通常以这种方式应用于正式化自动化推理系统的设计。关于概率论是推理系统基础的进一步论点可以在Pearl (1988)中找到。




概率的哲学基础以及它应该与我们认为应该真实的（逻辑意义上）之间存在某种关系，是由Cox（Jaynes, 2003）研究的。另一种思考方式是，如果我们对常识的描述足够精确，最终我们会构建出概率。E. T. Jaynes (1922–1998) 确定了三个数学标准，这些标准必须适用于所有合理的可能性：

“为了合理的推理，有必要将真理的离散真与假值扩展到连续的可能性值”（Jaynes, 2003）。  

1. 可能性的程度由实数表示。

2. 这些数字必须基于常识的规则。  

3. 结果的推理必须一致，这个术语“一致”有三个含义：

(a) 一致性或无矛盾性：当通过不同的方式得到相同的结果时，所有情况下都必须找到相同的可能性值。  

(b) 诚实性：所有可用的数据都必须被考虑。  

(c) 可重复性：如果我们对两个问题的知识状态相同，那么我们必须为两者分配相同的可能性程度。  

Cox–Jaynes 定理证明了这些可能性足以定义适用于可能性 $p$ 的普遍数学规则，这些规则在任意单调函数变换下适用。至关重要的是，这些规则是概率规则。  

注释。 在机器学习和统计学中，概率有两种主要的解释：贝叶斯解释和频率解释（Bishop, 2006; Efron 和 Hastie, 2016）。贝叶斯解释使用概率来指定用户对事件的不确定性程度。它有时被称为“主观概率”或“信念程度”。频率解释考虑了感兴趣事件的相对频率与发生的总事件数。事件的概率定义为当数据无限时事件的相对频率。$\diamondsuit$



一些机器学习文本在概率模型的使用上懒惰地使用符号和术语，这让人困惑。这本书也不例外。多个不同的概念都被统称为“概率分布”，读者往往需要从上下文中推断其含义。理解概率分布的一个技巧是检查我们是否在尝试建模某种分类（离散随机变量）或某种连续（连续随机变量）。我们在机器学习中解决的问题与我们考虑分类模型还是连续模型密切相关。




### 6.1.2 概率与随机变量  

讨论概率时，往往会混淆三个不同的概念。首先是概率空间的概念，它允许我们量化概率的概念。然而，我们通常并不直接处理这个基本的概率空间。相反，我们处理随机变量（第二个概念），它将概率转移到一个更方便（通常是数值）的空间。第三个概念是与随机变量相关的分布或法则。我们将在本节介绍前两个概念，并在第6.2节中扩展第三个概念。  

现代概率基于Kolmogorov（Grinstead 和 Snell, 1997; Jaynes, 2003）提出的公理集，引入了样本空间、事件空间和概率度量这三个概念。概率空间模型了一个具有随机结果的真实世界过程（通常称为实验）。




#### 样本空间 $\Omega$  

样本空间是实验所有可能结果的集合，通常用 $\Omega$ 表示。例如，连续抛掷两次硬币的样本空间为 { hh, tt, ht, th } ，其中“h”表示“正面”，“t”表示“反面”。  

#### 事件空间 $\mathcal{A}$  

样本空间 $\Omega$ 的子集 $A$ 在事件空间 $\mathcal{A}$ 中，如果在实验中我们可以观察到特定结果 $\omega\in\Omega$ 是否属于 $A$ 。事件空间 $\mathcal{A}$ 是通过考虑 $\Omega$ 的子集集合获得的，对于离散概率分布（第6.2.1节），$\mathcal{A}$ 往往是 $\Omega$ 的幂集。  

#### 概率 $P$  

对于每个事件 $A\in{\mathcal{A}}$ ，我们关联一个数字 $P(A)$ ，衡量事件发生的概率或信念程度。$P(A)$ 称为事件 $A$ 的概率。  

单个事件的概率必须位于区间 $[0,1]$ 内，样本空间 $\Omega$ 中所有结果的总概率必须为 1 ，即 $P(\Omega)=1$ 。给定一个概率空间 $(\Omega,{\mathcal{A}},P)$ ，我们希望使用它来模拟某些真实世界的现象。在机器学习中，我们通常避免明确引用概率空间，而是引用对感兴趣量的概率，我们用 $\mathcal{T}$ 表示。在本书中，我们将 $\mathcal{T}$ 称为“态”，并将 $\mathcal{T}$ 的元素称为状态。我们引入一个函数 $X:\Omega\rightarrow\mathcal{T}$ ，它将 $\Omega$ 中的元素（结果）映射到一个与兴趣量相关的量 $x$ ，一个在 $\mathcal{T}$ 中的值。这种从 $\Omega$ 映射到 $\mathcal{T}$ 的关联/映射称为随机变量。例如，在连续抛掷两次硬币并计算正面数量的情况下，随机变量 $X$ 映射到三个可能的结果：$X(\mathrm{hh})=2$ ，$X(\mathrm{ht})=1$ ，$X(\mathrm{th})=1$ ，和 $X(\mathrm{tt})=0$ 。在这种特定情况下，$\mathcal{T}=\{0,1,2\}$ ，我们对 $\mathcal{T}$ 中的元素的概率感兴趣。对于有限的样本空间 $\Omega$ 和有限的 $\mathcal{T}$ ，对应于随机变量 $X$ 的函数本质上是一个查找表。对于任何子集 $S\subseteq\mathcal{T}$ ，我们关联一个概率 $P_{X}(S)\,\in\,[0,1]$ （概率）到特定事件发生的概率，对应于随机变量 $X$ 。例6.1提供了术语的具体说明。




注释。 如上所述的样本空间 $\Omega$ 不幸地在不同的书籍中被赋予了不同的名称。另一个常见的 $\Omega$ 名称是“状态空间”（Jacod 和 Protter, 2004），但状态空间有时专用于指代动力系统中的状态（Hasselblatt 和 Katok, 2003）。描述 $\Omega$ 时有时使用的其他名称有：“样本描述空间”、“可能性空间”和“事件空间”。$\diamondsuit$




### 示例6.1  

这个玩具示例本质上是一个有偏硬币翻转示例。  

我们假设读者已经熟悉计算事件集交集和并集的概率。Walpole等人（2011）的第二章提供了概率的更温和介绍，包含许多示例。  

考虑一个统计实验，我们模拟一个游乐场游戏，其中从袋子中抽取两个硬币（带放回）。袋中有美国（用 $\S.$ 表示）和英国（用 £ 表示）的硬币。由于我们从袋子中抽取两个硬币，总共有四种可能的结果。这个实验的样本空间或样本空间 $\Omega$ 是 $(\mathbb{S},\,\mathbb{S})$ ,  $(\mathbb{S},$ $\mathcal{L}$) ,  $(\mathcal{L},\mathcal{L})$ ,  $(\mathcal{L},\mathcal{L})$ 。假设硬币袋的组成使得随机抽取一个 $S$ 的概率为 0.3。  

我们感兴趣的事件是重复抽取返回 £ 的总次数。我们定义一个随机变量 $X$ ，它将样本空间 $\Omega$ 映射到 $\mathcal{T}$ ，表示从袋子中抽取 £ 的次数。从前面的样本空间我们可以得到零 £ ，一个 £ 或两个 £ ，因此 $\mathcal{T}=\{0,1,2\}$ 。随机变量 $X$ （一个函数或查找表）可以表示为如下表格：

$$
\begin{array}{r}{X((\$,\$))=2}\\ {X((\$,\mathcal{L}))=1}\\ {X((\mathcal{L},\$))=1}\\ {X((\mathcal{L},\mathcal{L}))=0\,.}\end{array}
$$

由于我们在抽取第二个硬币之前返回第一个抽取的硬币，这意味着两次抽取是彼此独立的，我们将在第6.4.5节讨论这一点。注意，有两个实验结果映射到相同的事件，其中只有一项抽取返回 £ 。因此，随机变量 $X$ 的概率质量函数（第6.2.1节）给出为：

$$
\begin{array}{r l}&{P(X=2)=P((£,£))}\\ &{\qquad\qquad=P(£)\cdot P(£)}\\ &{\qquad\qquad=0.3\cdot0.3=0.09}\\ &{P(X=1)=P((£,\mathcal{L})\cup(\mathcal{L},£))}\\ &{\qquad\qquad=P((£,\mathcal{L}))+P((\mathcal{L},£))}\\ &{\qquad\qquad=0.3\cdot(1-0.3)+(1-0.3)\cdot0.3=0.42}\\ &{P(X=0)=P((\mathcal{L},\mathcal{L}))}\\ &{\qquad\qquad=P(\mathcal{L})\cdot P(\mathcal{L})}\\ &{\qquad\qquad=(1-0.3)\cdot(1-0.3)=0.49\,.}\end{array}
$$  

在计算中，我们等同了两个不同的概念，即 $X$ 的输出概率和 $\Omega$ 中的样本概率。例如，在（6.7）中，我们说 $P(X\,=\,0)\,=\,P(({\mathcal{L}},{\mathcal{L}}))$ 。考虑随机变量 $X:\Omega\rightarrow\tau$ 和一个子集 $S\subseteq\tau$ （例如，$\mathcal{T}$ 中的一个元素，例如掷两次硬币时获得一个头的结果）。令 $X^{-1}(S)$ 是 $S$ 通过 $X$ 的预像，即映射到 $S$ 的 $\Omega$ 中的元素集合；$\{\omega\,\in\,\Omega\,:\,X(\omega)\,\in\,S\}$ 。理解通过随机变量 $X$ 从事件空间 $\Omega$ 转换概率的方式是将其与 $S$ 的预像的概率关联起来（Jacod 和 Protter, 2004）。对于 $S\subseteq\mathcal{T}$ ，我们有符号：

$$
P_{X}(S)=P(X\in S)=P(X^{-1}(S))=P(\{\omega\in\Omega:X(\omega)\in S\})\,.
$$  

（6.8）等式的左侧是我们感兴趣的可能结果集（例如，$\$=1$ 的数量）的概率。通过随机变量 $X$ ，它将状态映射到结果，我们看到在（6.8）等式的右侧，这是 $\Omega$ 中具有特定属性（例如，$\${\mathcal{L}},\,{\mathcal{L}}\$）的状态集的概率。我们说随机变量 $X$ 根据特定的概率分布 $P_{X}$ 分布，它定义了事件与随机变量结果概率之间的概率映射。换句话说，函数 $P_{X}$ 或等价地 $P\circ X^{-1}$ 是随机变量 $X$ 的 概率分布 或 分布 。  

注释。 目标空间，即随机变量 $X$ 的范围 $\mathcal{T}$ ，用于表示概率空间的类型，即一个 $\mathcal{T}$ 随机变量。当 $\mathcal{T}$ 有限或可数无穷时，这称为离散随机变量（第6.2.1节）。对于连续随机变量（第6.2.2节），我们仅考虑 $\mathcal{T}=\mathbb{R}$ 或 $\mathcal{T}=\mathbb{R}^{D}$ 。$\diamondsuit$




### 6.1.3 统计学  

概率论和统计学经常被一起介绍，但它们关注的是不确定性不同的方面。通过对比它们所考虑的问题类型，我们可以区分它们。使用概率，我们可以考虑一个过程的模型，其中底层的不确定性由随机变量捕获，并使用概率规则推导出发生的事情。在统计学中，我们观察到某件事情已经发生，并试图找出解释观察结果的底层过程。从这个意义上说，机器学习在构建能够充分代表生成数据的过程的模型的目标上与统计学最为接近。我们可以使用概率规则来获得一些数据的最佳拟合模型。  

机器学习系统中的另一个方面是我们对泛化误差感兴趣（参见第8章）。这意味着我们实际上对系统在将来观察到的实例上的性能感兴趣，这些实例与我们迄今为止看到的实例并不完全相同。对将来性能的这种分析依赖于概率和统计学，其中大部分内容超出了本章的范围。对这一主题感兴趣的读者可以参考Boucheron等人（2013）和Shalev-Shwartz和Ben-David（2014）的书籍。在第8章中，我们将更多地了解统计学。

## 6.2 离散和连续概率  

概率质量函数  

累积分布函数  

单变量  

多变量  

联合概率  

概率质量函数  

让我们将注意力集中在第6.1节中介绍的事件概率的描述方式上。根据目标空间是离散还是连续，描述分布的方式自然不同。当目标空间 $\mathcal{T}$ 是离散的，我们可以说随机变量 $X$ 取特定值 $x\in\mathcal T$ 的概率，表示为 $P(X=x)$ 。对于离散随机变量 $X$ ，表达式 $P(X=x)$ 被称为概率质量函数。当目标空间 $\mathcal{T}$ 是连续的，例如实数线 $\mathbb{R}$ ，更自然的是指定随机变量 $X$ 在区间内的概率，表示为 $P(a\leqslant X\leqslant b)$ ，其中 $a<b$ 。通常，我们指定随机变量 $X$ 小于特定值 $x$ 的概率，表示为 $P(X\leqslant x)$ 。对于连续随机变量 $X$ ，表达式 $P(X\leqslant x)$ 被称为累积分布函数。我们将连续随机变量在第6.2.2节中讨论。在第6.2.3节中，我们将回顾术语并对比离散和连续随机变量。  

注释。 我们将使用短语 单变量 分布来指代一个随机变量（其状态表示为非粗体 $x_{.}$ ）的分布。我们将多于一个随机变量的分布称为 多变量 分布，并通常考虑一个随机变量向量（其状态表示为粗体 $\pmb{x}$ ）。$\diamondsuit$




### 6.2.1 离散概率  

当目标空间是离散的，我们可以想象多个随机变量的概率分布填充了一个（多维）数字数组。图6.2给出了一个示例。联合概率的目标空间是每个随机变量目标空间的笛卡尔积。我们将联合概率定义为两个值的共同条目：

$$
P(X=x_{i},Y=y_{j})=\frac{n_{i j}}{N}\,,
$$ 

其中 $n_{i j}$ 是状态为 $x_{i}$ 和 $y_{j}$ 的事件数量，$N$ 是总事件数量。联合概率是两个事件的交集的概率，即 $P(X=x_{i},Y=y_{j})=P(X=x_{i}\cap Y=y_{j})$ 。图6.2展示了离散概率分布的概率质量函数（pmf）。对于两个随机变量 $X$ 和 $Y$ ，概率

![](images/da5767bbbad987ae8f5a17dba9f14027916b835db0be055e58556f61fc490a3d.jpg)  
图6.2展示了离散双变量概率质量函数的可视化，包含随机变量 $X$ 和 $Y$ 。此图表改编自Bishop (2006)。

将 $X=x$ 和 $Y=y$ 的概率（懒惰地）写作 $p(x,y)$ 并称为联合概率。可以将概率视为一个函数，它接受状态 $x$ 和 $y$ 并返回一个实数，这就是我们写作 $p(x,y)$ 的原因。随机变量 $X$ 不考虑随机变量 $Y$ 的值取值为 $x$ 的边际概率（懒惰地）写作 $p(x)$ 。我们写作 $X\,\sim\,p(x)$ 表示随机变量 $X$ 根据 $p(x)$ 分布。如果我们只考虑 $X=x$ 的实例，则对于 $Y=y$ 的实例比例（条件概率）写作（懒惰地） $p(y\,|\,x)$ 。  

边际概率  

条件概率

### 示例6.2  

考虑两个随机变量 $X$ 和 $Y$ ，其中 $X$ 有五个可能的状态，$Y$ 有三个可能的状态，如图6.2所示。我们用 $n_{ij}$ 表示状态为 $X=x_{i}$ 和 $Y=y_{j}$ 的事件数量，并用 $N$ 表示总事件数量。值 $c_{i}$ 是第 $i$ 列的个体频率之和，即 $c_{i}=\sum_{j=1}^{3}n_{ij}$ 。同样，值 $r_{j}$ 是行总和，即 $r_{j}=\sum_{i=1}^{5}n_{ij}$ 。使用这些定义，我们可以紧凑地表示 $X$ 和 $Y$ 的分布。

每个随机变量的概率分布，即边际概率，可以被视为行或列的总和：

$$
P(X=x_{i})=\frac{c_{i}}{N}=\frac{\sum_{j=1}^{3}n_{ij}}{N}
$$  

和  

$$
P(Y=y_{j})=\frac{r_{j}}{N}=\frac{\sum_{i=1}^{5}n_{ij}}{N},
$$  

其中 $c_{i}$ 和 $r_{j}$ 分别是概率表中的第 $i$ 列和第 $j$ 行。按照惯例，对于有限事件的离散随机变量，我们假设概率总和为一，即  

$$
\sum_{i=1}^{5}P(X=x_{i})=1\quad{\mathrm{和}}\quad\sum_{j=1}^{3}P(Y=y_{j})=1\,。
$$  

条件概率是特定单元格中行或列的比例。例如，给定 $X$ 的 $Y$ 的条件概率是  

$$
P(Y=y_{j}\,|\,X=x_{i})=\frac{n_{ij}}{c_{i}}\,，
$$  

而给定 $Y$ 的 $X$ 的条件概率是  

$$
P(X=x_{i}\,|\,Y=y_{j})=\frac{n_{ij}}{r_{j}}\,。
$$  

分类变量  

在机器学习中，我们使用离散概率分布来建模分类变量，即取有限个无序值的变量。它们可以是分类特征，例如在用于预测个人薪水时的大学学位，或者分类标签，例如在进行手写识别时的字母表。离散分布也经常用于构建结合有限数量连续分布的概率模型（第11章）。




### 6.2.2 连续概率  

本节考虑实值随机变量，即我们考虑实数线 $\mathbb{R}$ 上的区间作为目标空间。在本书中，我们假装可以像处理有限状态的离散概率空间一样对实随机变量执行操作。然而，这种简化在两种情况下并不精确：当我们无限次重复某事时，以及当我们想要从区间中抽取一个点时。第一种情况出现在我们讨论机器学习中的泛化误差（第8章）。第二种情况出现在我们想要讨论连续分布，如高斯分布（第6.5节）时。为了我们的目的，这种不精确性允许我们对概率进行更简短的介绍。  

测量  

Borel $\sigma$ -代数 注释。 在连续空间中，有两个额外的技巧性问题，这些是直觉上难以理解的。首先，用于定义事件空间 $\mathcal{A}$ 的所有子集的集合不够良好行为。 $\mathcal{A}$ 需要被限制以在集合补集、集合交集和集合并集下表现出良好行为。其次，集合的大小（在离散空间中可以通过计数元素来获得）实际上很棘手。集合的大小称为其 测量 。例如，离散集合的基数，$\mathbb{R}_{2}$ 中区间的长度，以及 $\mathbb{R}^{d}$ 中区域的体积都是测量。在集合操作下表现出良好行为并具有拓扑的集合被称为 Borel $\sigma$ -代数。Betancourt详细介绍了从集合论构建概率空间的仔细构造，而没有被技术细节所困扰；参见 https://tinyurl.com/yb3t6mfd 。对于更精确的构造，我们参考Billingsley (1995) 和 Jacod 和 Protter (2004)。  

在本书中，我们考虑实值随机变量及其对应的Borel $\sigma$ -代数。我们将值在 $\mathbb{R}^{D}$ 的随机变量视为实值随机变量的向量。 $\diamondsuit$ 

### 定义6.1（概率密度函数） 。

函数 $f:\mathbb{R}^{D}\rightarrow\mathbb{R}$ 被称为概率密度函数（pdf）如果：

1. $\forall{\pmb{x}}\in\mathbb{R}^{D}:f({\pmb{x}})\geqslant 0$

2. 它的积分存在，并且  

$$
\int_{\mathbb{R}^{D}}f(\pmb{x})\mathrm{d}\pmb{x}=1\,。
$$

概率密度函数 pdf

对于离散随机变量的概率质量函数（pmf），（6.15）中的积分被替换为（6.12）。  

观察到概率密度函数是任何非负且积分等于一的函数。我们将随机变量 $X$ 与这个函数 $f$ 关联为：

$$
P(a\leqslant X\leqslant b)=\int_{a}^{b}f(x)\mathrm{d}x\,，
$$  

其中 $a,b\in\mathbb{R}$ 和 $x\in\mathbb{R}$ 是连续随机变量 $X$ 的结果。 $\pmb{x}\,\in\,\mathbb{R}^{D}$ 定义类似地考虑 $x\in\mathbb{R}$ 的向量。这种关联（6.16）被称为随机变量 $X$ 的 概率分布 或 分布 。  

注释。 与离散随机变量不同，连续随机变量 $X$ 取特定值 $P(X\,=\,x)$ 的概率为零。这就像试图在（6.16）中指定一个区间 $a=b$ 。$\diamondsuit$  

### 定义6.2（累积分布函数） 。
多变量实值随机变量 $X$ 的累积分布函数（cdf）给出为：

$P(X=x)$ 是一个测量值为零的集合。  

累积分布函数  

$$
F_{X}({\pmb{x}})=P(X_{1}\leqslant x_{1},.\,.\,,X_{D}\leqslant x_{D})\,，
$$  

其中 $X\,=\,[X_{1},\cdots,X_{D}]^{\top}$ ， $\pmb{x}\,=\,[x_{1},\cdots,x_{D}]^{\top}$ ，右侧表示随机变量 $X_{i}$ 取小于或等于 $x_{i}$ 的值的概率。  

cdf也可以表示为概率密度函数 $f(\pmb{x})$ 的积分，因此为：

存在没有对应pdf的cdf。  

$$
F_{X}({\pmb{x}})=\int_{-\infty}^{x_{1}}\cdot\cdot\cdot\int_{-\infty}^{x_{D}}f(z_{1},\cdots,z_{D})\mathrm{d}z_{1}\cdot\cdot\cdot\mathrm{d}z_{D}\,。
$$  

注释。 我们再次强调，当我们谈论分布时实际上有两类不同的概念。第一类是pdf（表示为 $f(x)$ ）的概念，它是一个非负函数，其总和为一。第二类是随机变量 $X$ 的概率分布，即随机变量 $X$ 与pdf $f(x)$ 的关联。$\diamondsuit$  

![](images/13c9e582002296b6e6774cd255f7de93247d9a00098bac2292622f1b337de517.jpg)  
图6.3（a）离散和（b）连续均匀分布的示例。有关分布的详细信息，请参见例6.3。  

![](images/2114020ec96b38126ee2c939aefcb65c8f7cb36b79165d817cafb93a3798b4e3.jpg)  

本书的大部分内容中，我们不会使用符号 $f(x)$ 和 $F_{X}(x)$ ，因为我们通常不需要区分pdf和cdf。然而，在第6.7节中，我们需要小心处理pdf和cdf。




### 6.2.3 离散分布与连续分布的对比  

均匀分布 请参阅第6.1.2节，概率是正数，总概率加起来为一。对于离散随机变量（见（6.12）），这意味着每个状态的概率必须位于区间 $[0,1]$ 内。然而，对于连续随机变量，归一化（见（6.15））并不意味着所有值的密度值小于或等于 1 。我们通过图6.3使用离散和连续随机变量的均匀分布来说明这一点。  

### 示例6.3  

我们考虑均匀分布的两个示例，其中每个状态发生的可能性相等。这个例子展示了离散分布与连续概率分布之间的某些差异。  

这些状态的实际值在这里没有意义，我们故意选择了数字来强调我们不希望使用（并且应该忽略）状态的顺序。  

令 $Z$ 是一个离散均匀随机变量，有三个状态 $\{z=-1.1,z=0.3,z=1.5\}$ 。概率质量函数可以表示为概率值的表格：  

$$
\begin{array}{r}{z\ \ -1.1\ 0.3\ \ 1.5\ }\\ {P(Z=z)\sqrt{\begin{array}{l}{{\frac{1}{3}}}\end{array}\left|\begin{array}{l}{{\frac{1}{3}}}\end{array}\right|\begin{array}{l}{{\frac{1}{3}}}\end{array}}}\end{array}
$$  

或者，我们可以将其视为图形（图6.3(a)），其中我们利用事实，状态可以在 $x$ 轴上定位，而 $y$ 轴表示特定状态的概率。图6.3(a)中的 $y$ 轴故意被拉伸，使其与图6.3(b)中的相同。  

令 $X$ 是一个连续随机变量，其值在范围 $0.9\leqslant X\leqslant1.6$ 内，如图6.3(b)所示。观察到密度的高度可以大于 1 。然而，需要满足  

$$
\int_{0.9}^{1.6}p(x)\mathrm{d}x=1\,。
$$  

注释。 关于离散概率分布，还有一个细微的差别。状态 $z_{1},\dots,z_{d}$ 在原则上没有结构，即通常没有比较它们的方法，例如 $z_{1}=\mathrm{red},z_{2}=\mathrm{green},z_{3}=\mathrm{blue}$ 。然而，在许多机器学习中，离散状态取数值值，例如 $z_{1}=-1.1,z_{2}=0.3,z_{3}=1.5,$ ，我们可以说 $z_{1}<z_{2}<z_{3}$ 。假设数值值的离散状态特别有用，因为我们经常考虑随机变量的期望值（第6.4.1节）。$\diamondsuit$  

不幸的是，机器学习文献使用符号和术语隐藏了样本空间 $\Omega$ 、目标空间 $\mathcal{T}$ 和随机变量 $X$ 之间的区别。对于 $x$ ，即随机变量 $X$ 的可能结果的集合，即 $x\in\mathcal T$ ， $p(x)$ 表示随机变量 $X$ 有结果 $x$ 的概率。对于离散随机变量，这写为 $P(X\,=\,x)$ ，这被称为概率质量函数。pmf通常被称为“分布”。对于连续变量， $p(x)$ 被称为概率密度函数（通常称为密度）。为了使事情更加混乱，累积分布函数 $P(X\leqslant x)$ 也经常被称为“分布”。在本章中，我们将使用符号 $X$ 来指代单变量和多变量随机变量，并分别用 $x$ 和 $\pmb{x}$ 表示状态。我们总结了术语表6.1。  

注释。 我们不仅在离散概率质量函数上，而且在连续概率密度函数上使用“概率分布”这一表达，尽管这在技术上是不正确的。与大多数机器学习文献一致，我们也依赖于上下文来区分“概率分布”这一短语的不同用途。$\diamondsuit$

## 6.3 求和规则、乘积规则和贝叶斯定理  

我们将概率论视为逻辑推理的扩展。正如我们在第6.1.1节中讨论的，这里呈现的概率规则自然地遵循满足期望（Jaynes, 2003, 第2章）。概率建模（第8.4节）为设计机器学习方法提供了原则性的基础。一旦我们定义了与数据和问题不确定性相对应的概率分布（第6.2节），实际上只有两个基本规则，求和规则和乘积规则。  

这两个规则自然地（Jaynes, 2003）从我们在第6.1.1节中讨论的要求中产生。求和规则  

边际化性质  

请参阅（6.9），$p(\pmb{x},\pmb{y})$ 是两个随机变量 $x$, $y$ 的联合分布。分布 $p(\pmb{x})$ 和 $p(\pmb{y})$ 是相应的边际分布，而 $p(\pmb{y}\,|\,\pmb{x})$ 是给定 $x$ 的 $y$ 条件分布。根据第6.2节中离散和连续随机变量的边际和条件概率的定义，我们现在可以呈现概率理论中的两个基本规则。第一个规则，求和规则，表明  

$$
p({\pmb{x}})=\left\{\begin{array}{l l}{\displaystyle\sum_{{\pmb y}\in{\mathscr Y}}p({\pmb{x}},{\pmb y})\quad}&{\mathrm{if~}{\pmb y}\mathrm{~is~discrete}}\\ {\displaystyle\int_{\mathscr Y}p({\pmb{x}},{\pmb y})\mathrm{d}{\pmb y}\quad}&{\mathrm{if~}{\pmb y}\mathrm{~is~continuous}}\end{array}\right.,
$$  

其中 $y$ 是目标空间中随机变量 $Y$ 的状态。这意味着我们对随机变量 $Y$ 的状态集 $y$ 进行求和（或积分）。求和规则也被称为 边际化性质 。求和规则将联合分布与边际分布相关联。一般来说，当联合分布包含超过两个随机变量时，求和规则可以应用于任何随机变量的子集，从而产生一个可能包含一个以上随机变量的边际分布。更具体地说，如果 $\pmb{x}=[x_{1},\cdots,x_{D}]^{\top}$ ，我们通过重复应用求和规则来获得边际  

$$
p(x_{i})=\int p(x_{1},\dots,x_{D})\mathrm{d}\pmb{x}_{\backslash i}
$$  

其中 $\backslash i$ 读作“所有除了 $i$ ”，表示我们对除了 $x_{i}$ 之外的所有随机变量进行积分/求和。  

注释。 许多概率建模的计算挑战都归因于应用求和规则。当变量很多或离散变量的状态很多时，求和规则最终导致进行高维求和或积分。进行高维求和或积分通常在计算上是困难的，因为没有已知的多项式时间算法可以精确计算它们。$\diamondsuit$ 乘积规则  

第二个规则，称为 乘积规则 ，通过  

$$
p(\pmb{x},\pmb{y})=p(\pmb{y}\,|\,\pmb{x})p(\pmb{x})\,.
$$  

乘积规则可以解释为每个两个随机变量的联合分布都可以分解（写为乘积）为两个其他分布的乘积。两个因子是第一个随机变量的边际分布 $p(\pmb{x})$ ，以及第二个随机变量给定 $x$ 的条件分布 $p(\pmb{y}\,|\,\pmb{x})$ 。由于 $p(\pmb{x},\pmb{y})$ 中随机变量的顺序是任意的，乘积规则也暗示 $p(\pmb{x},\pmb{y})=p(\pmb{x}\,|\,\pmb{y})p(\pmb{y})$ 。为了精确，（6.22）以离散随机变量的概率质量函数的形式表达。对于连续随机变量，乘积规则以概率密度函数（第6.2.3节）的形式表达。  

在机器学习和贝叶斯统计中，我们通常对给定观察到其他随机变量的情况下，根据我们对未观察到的（潜在的）随机变量 $x$ 的先验知识和 $x$ 与第二个可观察随机变量 $y$ 之间的关系 $p(\pmb{y}\,|\,\pmb{x})$ ，我们对 $x$ 进行推理。如果观察到 $y$ ，我们可以使用 贝叶斯定理 来根据观察到的 $y$ 的值对 $x$ 进行一些结论。 贝叶斯定理 也称为 贝叶斯规则 或 贝叶斯定律  

贝叶斯定理 贝叶斯规则 贝叶斯定律  

$$
\underbrace{p(\pmb{x}\,|\,\pmb{y})}_{\mathrm{posterior}}=\frac{\overbrace{p(\pmb{y}\,|\,\pmb{x})}^{likelyhood}\overbrace{p(\pmb{x})}^{prior}}{\underbrace{p(\pmb{y})}_{\mathrm{evidence}}}
$$

它是（6.22）中乘积规则的直接结果，因为  

$$
p(\pmb{x},\pmb{y})=p(\pmb{x}\,|\,\pmb{y})p(\pmb{y})
$$  

和  

$$
p(\pmb{x},\pmb{y})=p(\pmb{y}\,|\,\pmb{x})p(\pmb{x})
$$  

因此  

$$
p(\pmb{x}\,|\,\pmb{y})p(\pmb{y})=p(\pmb{y}\,|\,\pmb{x})p(\pmb{x})\iff p(\pmb{x}\,|\,\pmb{y})=\frac{p(\pmb{y}\,|\,\pmb{x})p(\pmb{x})}{p(\pmb{y})}\,.
$$  

在（6.23）中， $p(\pmb{x})$ 是 先验 ，它封装了我们对未观察到（潜在的）变量 $x$ 的主观先验知识，在观察任何数据之前。我们可以选择任何对我们有意义的先验，但必须确保先验在所有可能的 $x$ 上都有非零pdf（或pmf），即使它们非常罕见。  

  $p(\pmb{y}\,|\,\pmb{x})$ 描述了 $\pmb{x}$ 和 $\pmb{ y}$ 之间的关系，对于离散概率分布，它是如果我们知道潜在变量 $\pmb{x}$ 的数据 $\pmb{ y}$ 的概率。请注意，可能性不是在 $\pmb{x}$ 中，而只是在 $\pmb{ y}$ 中。我们将 $p(\pmb{y}\,|\,\pmb{x})$ 称为 “ $\pmb{x}$ 的可能性”（给定 $\mathbf{\lambda}_{\mathbf{\lambda}}$ ）” 或 “ $\pmb{ y}$ 给定 $\scriptstyle{\mathbf{\mathit{x}}}^{\star}$ 的可能性” 但永远不是 $\pmb{ y}$ 的可能性（MacKay, 2003）。  

  $p(\pmb{x}\,|\,\pmb{y})$ 是贝叶斯统计中的量，因为它精确地表达了我们感兴趣的内容，即在观察 $\pmb{ y}$ 后我们对 $x$ 的了解。  

先验  

可能性 贝叶斯定理有时也被称为 “测量模型”。  

后验  

量  

$$
p(\pmb{y}):=\int p(\pmb{y}\,|\,\pmb{x})p(\pmb{x})\mathrm{d}\pmb{x}=\mathbb{E}_{X}[p(\pmb{y}\,|\,\pmb{x})]
$$  

边际可能性证据  

贝叶斯定理也被称为 “概率逆”。概率逆是 边际可能性 /证据 。（6.27）右侧使用了我们在第6.4.1节中定义的期望运算符。根据定义，边际可能性对 $x$ 进行积分，因此边际可能性独立于 $x$ ，并确保后验 $p(\pmb{x}\,|\,\pmb{y})$ 正则化。边际可能性也可以解释为期望的可能性，其中我们以先验 $p(\pmb{x})$ 为条件进行期望。除了后验的正则化之外，边际可能性在贝叶斯模型选择中也扮演着重要角色，我们将在第8.6节中讨论。由于（8.44）中的积分，证据通常很难计算。  

贝叶斯定理（6.23）允许我们反转给定可能性与 $\pmb{x}$ 和 $\pmb{ y}$ 之间的关系。因此，贝叶斯定理有时被称为 “概率逆”。我们将在第8.4节中进一步讨论贝叶斯定理。  

注释。 在贝叶斯统计中，后验分布是我们感兴趣的量，因为它封装了来自先验和数据的所有可用信息。而不是携带后验，我们可能专注于后验的一些统计量，例如后验的最大值，我们将在第8.3节中讨论。然而，专注于后验的一些统计量会导致信息丢失。如果我们从更大的上下文考虑，那么后验可以在决策系统中使用，并且拥有完整的后验可以非常有用，并导致对干扰具有鲁棒性的决策。例如，在基于模型的强化学习的上下文中，Deisenroth等人（2015）表明，使用可能过渡函数的完整后验分布导致非常快速（数据/样本高效）学习，而专注于后验的最大值会导致一致的失败。因此，拥有完整的后验对于下游任务非常有用。在第9章中，我们将在线性回归的背景下继续这一讨论。$\diamondsuit$

## 6.4 统计量和独立性  

我们经常对随机变量的集合进行总结，并比较一对随机变量。随机变量的统计量是一个确定性函数，该函数基于该随机变量。分布的统计量提供了一种有用的视角来了解随机变量的行为，并如名称所示，提供了总结和描述分布的数字。我们将描述均值和协方差，这两个广为人知的统计量。然后我们将讨论比较一对随机变量的两种方法：首先，如何说两个随机变量是独立的；其次，如何计算它们之间的内积。  

### 6.4.1 均值和协方差  

均值和（协）方差通常用于描述概率分布的属性（期望值和分散）。在第6.6节中，我们将看到一个有用的分布家族（称为指数家族），其中随机变量的统计量捕获所有可能的信息。  

期望值的概念在机器学习中至关重要，概率本身的基石概念可以从中推导出来（Whittle, 2000）。  

### 定义6.3（期望值） 。

一维连续随机变量 $X\sim p(x)$ 的期望函数 $g:\mathbb{R}\to$ 期望值 $\mathbb{R}$ 定义为  

$$
{\mathbb E}_{X}[g(x)]=\int_{\mathcal X}g(x)p(x)\mathrm{d}x\,。
$$  

相应地，离散随机变量 $X\sim p(x)$ 的函数 $g$ 的期望值给出为  

$$
\mathbb{E}_{X}[g(x)]=\sum_{x\in\mathcal{X}}g(x)p(x)\,，
$$  

其中 $\mathcal{X}$ 是随机变量 $X$ 的可能结果（目标空间）的集合。  

在本节中，我们考虑离散随机变量具有数值结果。这可以通过观察函数 $g$ 接受实数作为输入来看到。  

注释。 我们将多元随机变量 $X$ 视为有限向量的单变量随机变量 $[X_{1},\ldots,X_{D}]^{\top}$ 。对于多元随机变量，我们以元素方式定义期望值  

$$
\mathbb{E}_{X}[g({\pmb{x}})]=\left[\begin{array}{c}{\mathbb{E}_{X_{1}}[g(x_{1})]}\\ {\vdots}\\ {\mathbb{E}_{X_{D}}[g(x_{D})]}\end{array}\right]\in\mathbb{R}^{D}\,，
$$  

其中下标 $\mathbb{E}_{X_{d}}$ 指示我们正在对向量 $\pmb{x}$ 的第 $d$ 个元素取期望值。$\diamondsuit$  

定义6.3定义了符号 $\mathbb{E}_{X}$ 的含义，即表示我们应该对概率密度（对于连续分布）或所有状态的总和（对于离散分布）进行积分的运算符。均值（定义6.4）的定义是期望值的一个特殊情况，通过选择 $g$ 为恒等函数获得。  

### 定义6.4（均值） 。

多维随机变量 $X$ 的均值，状态 $\pmb{x}\in\mathbb{R}^{D}$ ，是一个平均值，并定义为  

$$
\mathbb{E}_{X}[{\pmb{x}}]=\left[\begin{array}{c}{\mathbb{E}_{X_{1}}[x_{1}]}\\ {\vdots}\\ {\mathbb{E}_{X_{D}}[x_{D}]}\end{array}\right]\in\mathbb{R}^{D}\,，
$$  

其中  

$$
\mathbb{E}_{X_{d}}[x_{d}]:={\left\{\begin{array}{l l}{\displaystyle\int_{X}x_{d}p(x_{d})\mathrm{d}x_{d}}&{{\mathrm{如果~}}X{\mathrm{~是~连续~随机~变量}}}\\ {\displaystyle\sum_{x_{i}\in X}x_{i}p(x_{d}=x_{i})}&{{\mathrm{如果~}}X{\mathrm{~是~离散~随机~变量}}}\end{array}\right.}
$$  

对于 $d\,=\,1,\cdot\,\cdot\,\cdot\,,D_{i}$ ，其中下标 $d$ 指示向量 $\pmb{x}$ 的对应维度。积分或总和是目标空间中随机变量 $X$ 的状态 $\mathcal{X}$ 上的。  

中位数  

在一维中，有两个其他直观的“平均”概念，即中位数和模数。中位数是排序值的“中间”值，即 $50\%$ 的值大于中位数， $50\%$ 的值小于中位数。这个想法可以通过考虑cdf（定义6.2）为 0 . 5 来推广到连续值。对于分布，如果它们不对称或有长尾，则中位数提供了一个更接近人类直觉的典型值，而不是均值。此外，中位数比均值对异常值更稳健。将中位数推广到更高维度并不简单，因为没有明显的方法在多于一个维度上“排序”（Hallin et al., 2010；Kong 和 Mizera, 2012）。模数是最常出现的值。对于离散随机变量，模数定义为出现频率最高的值。对于连续随机变量，模数定义为密度 $p(\pmb{x})$ 中的峰值。特定密度 $p(\pmb{x})$ 可能有多个模数，并且在高维分布中可能有大量模数。因此，找到分布的所有模数可能是计算上具有挑战性的。

### 示例6.4  

考虑图6.4中所示的二维分布：  

$$
p(x) = 0.4 \mathcal{N} \left( x \mid \begin{bmatrix} 10 \\ 2 \end{bmatrix}, \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \right) + 0.6 \mathcal{N} \left( x \mid \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 8.4 & 2.0 \\ 2.0 & 1.7 \end{bmatrix} \right)
$$

我们将在第6.5节中定义高斯分布 ${\mathcal{N}}({\boldsymbol{\mu}},\,{\boldsymbol{\sigma}}^{2})$ 。还展示了每个维度的对应边际分布。观察到分布是双模态的（有两个模数），但其中一个边际分布是单模态的（有一个模数）。水平双模态单变量分布说明了均值和中位数可能彼此不同。虽然定义两个维度的中位数为每个维度的中位数的串联似乎很诱人，但无法定义二维点的顺序使得它变得困难。当我们说“无法定义顺序”时，意味着有不止一种方法来定义关系 $<$使得 $\begin{bmatrix}3 \\0\end{bmatrix}< \begin{bmatrix}2 \\3\end{bmatrix}$。  

![](images/873a3df1fbd18cec5f6b2ca440d822aa6d40ee7824da7675708fbd24d4d0a6bb.jpg)  
图6.4展示了二维数据集的均值、模数和中位数，以及其边际密度。  

注释。 期望值（定义6.3）是一个线性运算符。对于实值函数 $f({\pmb{x}})=a g({\pmb{x}})+b h({\pmb{x}})$ ，其中 $a,b\in\mathbb{R}$ 和 $\pmb{x}\in\mathbb{R}^{D}$ ，我们得到  

$$
\begin{array}{l}{\displaystyle\mathbb{E}_{X}\left[f(\pmb{x})\right]=\int f(\pmb{x})p(\pmb{x})\mathrm{d}\pmb{x}}\\ {\displaystyle\qquad\qquad=\int[a g(\pmb{x})+b h(\pmb{x})]p(\pmb{x})\mathrm{d}\pmb{x}}\\ {\displaystyle\qquad\quad=a\int g(\pmb{x})p(\pmb{x})\mathrm{d}\pmb{x}+b\int h(\pmb{x})p(\pmb{x})\mathrm{d}\pmb{x}}\\ {\displaystyle\qquad\quad=a\mathbb{E}_{X}[g(\pmb{x})]+b\mathbb{E}_{X}[h(\pmb{x})]\,.}\end{array}
$$  

对于两个随机变量，我们可能希望描述它们之间的对应关系。协方差直观地表示了随机变量彼此依赖的概念。  

协方差  

术语：多元随机变量的协方差 $\mathrm{Cov}[x,y]$ 有时被称为交叉协方差，协方差指的是 $\mathrm{Cov}[\boldsymbol{x},\boldsymbol{x}]$ 。  

方差标准差  

协方差




### 定义6.5（一维协方差） 。
两个一维随机变量 $X,Y\,\in\,\mathbb{R}$ 之间的协方差给出为它们各自均值的偏差的期望乘积，即  

$$
\operatorname{Cov}_{X,Y}[x,y]:=\mathbb{E}_{X,Y}\big[(x-\mathbb{E}_{X}[x])(y-\mathbb{E}_{Y}[y])\big]\,。
$$  

注释。当期望或协方差关联的随机变量通过其参数明显时，下标通常被省略（例如， $\mathbb{E}_{X}[x]$ 经常写为 $\mathbb{E}[x]$ ）。$\diamondsuit$  

通过使用期望的线性性质，定义6.5中的表达式可以重写为期望值的乘积减去乘积的期望值，即  

$$
\begin{array}{r}{\mathrm{Cov}[x,y]=\mathbb{E}[x y]-\mathbb{E}[x]\mathbb{E}[y]\,.}\end{array}
$$  

变量与自身 $\mathrm{Cov}[\boldsymbol{x},\boldsymbol{x}]$ 的协方差称为 方差 并用 $\mathbb{V}_{X}[x]$ 表示。方差的平方根称为 标准差 ，通常表示为 $\sigma(x)$ 。协方差的概念可以推广到多元随机变量。  

### 定义6.6（多元协方差） 。
如果我们考虑两个多元随机变量 $X$ 和 $Y$ 分别具有 $\pmb{x}\,\in\,\mathbb{R}^{D}$ 和 $\pmb{y}\in\mathbb{R}^{E}$ 的状态，那么 $X$ 和 $Y$ 之间的协方差定义为  

$$
\mathrm{Cov}[\pmb{x},\pmb{y}]=\mathbb{E}[\pmb{x}\pmb{y}^{\top}]-\mathbb{E}[\pmb{x}]\mathbb{E}[\pmb{y}]^{\top}=\mathrm{Cov}[\pmb{y},\pmb{x}]^{\top}\in\mathbb{R}^{D\times E}\,。
$$  

定义6.6可以应用于两个参数中的相同多元随机变量，这产生了一个直观上捕捉随机变量“散布”的有用概念。对于多元随机变量，方差描述了随机变量各个维度之间的关系。  

### 定义6.7（方差） 。
随机变量 $X$ 的方差，状态 $\pmb{x}\in\mathbb{R}^{D}$ 和均值向量 $\pmb{\mu}\in\mathbb{R}^{D}$ 定义为  

$$
\begin{array}{r l}&{\mathbb{V}_{X}[{\pmb{x}}]=\operatorname{Cov}_{X}[{\pmb{x}},{\pmb{x}}]}\\ &{\qquad=\mathbb{E}_{X}[({\pmb{x}}-{\pmb\mu})({\pmb{x}}-{\pmb\mu})^{\top}]=\mathbb{E}_{X}[{\pmb{x}}{\pmb{x}}^{\top}]-\mathbb{E}_{X}[{\pmb{x}}]\mathbb{E}_{X}[{\pmb{x}}]^{\top}}\\ &{\qquad=\left[\begin{array}{c c c c c}{\operatorname{Cov}[x_{1},x_{1}]}&{\operatorname{Cov}[x_{1},x_{2}]}&{\dots}&{\operatorname{Cov}[x_{1},x_{D}]}\\ {\operatorname{Cov}[x_{2},x_{1}]}&{\operatorname{Cov}[x_{2},x_{2}]}&{\dots}&{\operatorname{Cov}[x_{2},x_{D}]}\\ {\vdots}&{\vdots}&{\ddots}&{\vdots}\\ {\operatorname{Cov}[x_{D},x_{1}]}&{\dots}&{\dots}&{\operatorname{Cov}[x_{D},x_{D}]}\end{array}\right].}\end{array}
$$  

协方差矩阵  

边际  

$( 8c)$ 中的 $D\times D$ 矩阵称为多元随机变量 $X$ 的 协方差矩阵 。协方差矩阵是对称的且非负半定的，并告诉我们数据的散布情况。在其对角线上，协方差矩阵包含边际的 方差  

![](images/1db43c6b914bb86495e48dbe991aae401d380474155b6459e523696efa107d24.jpg)  
(a) $x$ 和 $y$ 是负相关的。  

![](images/b1978fb2a2f857213af31b6820af3c48c8a62b020ae690b97372ad99af94f6f4.jpg)  
(b) $x$ 和 $y$ 是正相关的。  

$$
p(x_{i})=\int p(x_{1},\dots,x_{D})\mathrm{d}x_{\backslash i}\,，
$$  

其中 $\leftrightsquigarrow i"$ 表示“除了 $i'$ 的所有变量和 $i'$ 的条目是交叉协方差 项 $\mathrm{Cov}[x_{i},x_{j}]$ 为 $i,j=1,\cdot\cdot\cdot,D,$ $i\neq j$ ̸ 。交叉协方差  

注释。在本书中，我们通常假设协方差矩阵是正定的，以提供更好的直觉。因此，我们不讨论导致正半定（低秩）协方差矩阵的边缘情况。$\diamondsuit$  

当我们想要比较不同对随机变量之间的协方差时，发现每个随机变量的方差会影响协方差的值。协方差的规范化版本称为 相关性 。  

### 定义6.8（相关性） 。
两个随机变量 $X,Y$ 之间的 相关性 给出为  

$$
\operatorname{corr}[x,y]={\frac{\operatorname{Cov}[x,y]}{\sqrt{\mathbb{V}[x]\mathbb{V}[y]}}}\in\left[-1,1\right].
$$ 

相关矩阵是标准化随机变量的协方差矩阵， $x/\sigma(x)$ 。换句话说，相关矩阵中的每个随机变量都通过其标准差（方差的平方根）进行划分。  

协方差（和相关性）表明两个随机变量之间的关系；见图6.5。正相关 $\operatorname{corr}[x,y]$ 意味着当 $x$ 增长时， $y$ 也预期增长。负相关意味着当 $x$ 增加时， $y$ 减少。



### 6.4.2 实验均值和协方差  

关键字：实验均值样本均值  实验均值  实验协方差  

第6.4.1节中的定义通常也称为 总体均值和协方差 ，因为它指的是总体的真正统计量。在机器学习中，我们需要从数据的实验观察中学习。考虑一个随机变量 $X$ 。从总体均值和协方差到实现实验统计量的两个概念步骤如下。首先，我们利用我们有有限数据集（大小为 $N$ ）的事实，构建一个实验统计量，它是有限个相同随机变量的函数，$X_{1},\dots,X_{N}$ 。其次，我们观察数据，即我们查看每个随机变量的实现 $x_{1},\cdots,x_{N}$ ，并应用实验统计量。  

在本书中，我们使用实验协方差，这是一个有偏估计。无偏（有时称为校正）协方差在分母中具有因子 $N-1$ 而不是 $N$ 。推导在本章末尾作为练习。  

具体来说，对于均值（定义6.4），给定特定的数据集，我们可以获得均值的估计，这被称为 实验均值 或样本均值。对于实验协方差也是如此。  

### 定义6.9（实验均值和协方差） 。

实验均值向量是每个变量观察结果的算术平均值，并定义为  

$$
\bar{\pmb{x}}:=\frac{1}{N}\sum_{n=1}^{N}\pmb{x}_{n}\,，
$$  

其中 $\pmb{x}_{n}\in\mathbb{R}^{D}$ 。  

类似于实验均值， 实验协方差 矩阵是一个 $D\!\times\!D$ 矩阵  

$$
\pmb{\Sigma}:=\frac{1}{N}\sum_{n=1}^{N}(\pmb{x}_{n}-\bar{\pmb{x}})(\pmb{x}_{n}-\bar{\pmb{x}})^{\top}.
$$ 

为了计算特定数据集的统计量，我们会使用实现（观察） $\pmb{x}_{1},\dots,\pmb{x}_{N}$ 并使用（6.41）和（6.42）。实验协方差矩阵是对称的，非负半定（见第3.2.3节）。


### 6.4.3 三种表达式描述方差  

我们现在专注于单个随机变量 $X$ ，并使用前面的实验公式推导出方差的三种可能表达式。对于以下推导，除了需要处理积分外，对于总体方差的情况与定义6.5中给出的协方差定义相对应，是随机变量 $X$ 与其期望值 $\mu$ 的平方偏差的期望，即  

$$
\mathbb{V}_{X}[x]:=\mathbb{E}_{X}[(x-\mu)^{2}]\,。
$$  

在(6.43)中的期望和均值 $\mu\,=\,\mathbb{E}_{X}(x)$ 是根据(6.32)计算的，这取决于随机变量 $X$ 是离散的还是连续的。在(6.43)中表达的方差是新随机变量 $Z:=(X-\mu)^{2}$ 的均值。  

当我们根据(6.43)的实验估计方差时，需要采用两步算法：首先通过数据计算均值 $\mu$ ，使用(6.41)，然后使用这个估计值 $\hat{\mu}$ 计算方差。实际上，通过重新排列项，我们可以避免两步。(6.43)中的公式可以转换为所谓的 原始分数方差公式 ：  

$$
\mathbb{V}_{X}[x]=\mathbb{E}_{X}[x^{2}]-\left(\mathbb{E}_{X}[x]\right)^{2}\,。
$$ 

(6.44)中的表达式可以被记住为“均值的平方减去均值的平方”。由于我们可以同时累积 $x_{i}$ （用于计算均值）和 $x_{i}^{2}$ ，因此可以一次通过数据计算出这个表达式，其中 $x_{i}$ 是第 $i$ 个观察值。不幸的是，如果以这种方式实现，可能会导致浮点运算中的数值不稳定。原始分数版本的方差在机器学习中可能很有用，例如，在推导偏差-方差分解（Bishop, 2006）。  

理解方差的第三种方式是将其视为所有观察对之间差值的总和。考虑随机变量 $X$ 的实现样本 $x_{1},\dots,x_{N}$ ，我们计算 $x_{i}$ 和 $x_{j}$ 之间的平方差。通过展开平方，我们可以证明 $N^{2}$ 个对差的总和是观察值的实验方差：  

原始分数方差公式  

如果(6.44)中的两个项非常大且大致相等，我们可能会在浮点运算中遭受不必要的数值精度损失。  

$$
{\frac{1}{N^{2}}}\sum_{i,j=1}^{N}(x_{i}-x_{j})^{2}=2\left[{\frac{1}{N}}\sum_{i=1}^{N}x_{i}^{2}-\left({\frac{1}{N}}\sum_{i=1}^{N}x_{i}\right)^{2}\right]\,。
$$  

我们看到(6.45)是(6.44)的原始分数表达式的两倍。这意味着我们可以将 $N^{2}$ 个对距离（它们总共有 $N^{2}$ 个）表示为从均值（总共有 $N$ 个）的偏差的总和。从几何角度来看，这意味着对距离和点集中心的距离之间存在等价性。从计算角度来看，这意味着通过计算均值（求和中的 $N$ 项）和然后计算方差（再次 $N$ 项求和），我们可以获得一个表达式（(6.45)的左侧），它有 $N^{2}$ 项。

### 6.4.4 随机变量的求和与变换  

我们可能想要建模一个不能很好地由教科书分布解释的现象（我们在第6.5节和6.6节中引入了一些），因此可能会对随机变量进行简单的操作（例如，将两个随机变量相加）。  

考虑两个随机变量 $X,Y$ ，其状态分别为 $\pmb{x},\pmb{y}\in\mathbb{R}^{D}$ 。则：  

$$
\begin{array}{r l}&{\mathbb{E}[{\pmb{x}}+{\pmb y}]=\mathbb{E}[{\pmb{x}}]+\mathbb{E}[{\pmb y}]}\\ &{\mathbb{E}[{\pmb{x}}-{\pmb y}]=\mathbb{E}[{\pmb{x}}]-\mathbb{E}[{\pmb y}]}\\ &{\mathbb{V}[{\pmb{x}}+{\pmb y}]=\mathbb{V}[{\pmb{x}}]+\mathbb{V}[{\pmb y}]+\mathrm{Cov}[{\pmb{x}},{\pmb y}]+\mathrm{Cov}[{\pmb y},{\pmb{x}}]}\\ &{\mathbb{V}[{\pmb{x}}-{\pmb y}]=\mathbb{V}[{\pmb{x}}]+\mathbb{V}[{\pmb y}]-\mathrm{Cov}[{\pmb{x}},{\pmb y}]-\mathrm{Cov}[{\pmb y},{\pmb{x}}]\,.}\end{array}
$$  

均值和（协）方差在随机变量的线性变换时表现出一些有用的性质。考虑一个具有均值 $\pmb{\mu}$ 和协方差矩阵 $\pmb{\Sigma}$ 的随机变量 $X$ ，以及 $x$ 的（确定性）线性变换 ${\pmb y}\ =\ A{\pmb{x}}+b$ 。则 ${\pmb y}$ 自身是一个随机变量，其均值向量和协方差矩阵由以下给出：  

$$
\begin{array}{r l}&{\mathbb{E}_{Y}[{\pmb y}]=\mathbb{E}_{X}[{\pmb A}{\pmb{x}}+{\pmb{b}}]={\pmb A}\mathbb{E}_{X}[{\pmb{x}}]+{\pmb{b}}={\pmb A}{\pmb\mu}+{\pmb{b}}\,,}\\ &{\mathbb{V}_{Y}[{\pmb y}]=\mathbb{V}_{X}[{\pmb A}{\pmb{x}}+{\pmb{b}}]=\mathbb{V}_{X}[{\pmb A}{\pmb{x}}]={\pmb A}\mathbb{V}_{X}[{\pmb{x}}]{\pmb A}^{\top}={\pmb A}{\pmb\Sigma}{\pmb A}^{\top}\,,}\end{array}
$$  

这可以通过直接使用均值和协方差的定义来证明。  

分别。此外，  

$$
\begin{array}{r l}&{\mathrm{Cov}[\pmb{x},\pmb{y}]=\mathbb{E}[\pmb{x}(\pmb{A x}+\pmb{b})^{\top}]-\mathbb{E}[\pmb{x}]\mathbb{E}[\pmb{A x}+\pmb{b}]^{\top}}\\ &{\qquad\qquad=\mathbb{E}[\pmb{x}]\pmb{b}^{\top}+\mathbb{E}[\pmb{x x}^{\top}]\pmb{A}^{\top}-\pmb{b}^{\top}-\pmb{\mu}\pmb{\mu}^{\top}\pmb{A}^{\top}}\\ &{\qquad\qquad=\pmb{\mu}\pmb{b}^{\top}-\pmb{\mu}\pmb{b}^{\top}+\big(\mathbb{E}[\pmb{x x}^{\top}]-\pmb{\mu}\pmb{\mu}^{\top}\big)\pmb{A}^{\top}}\\ &{\qquad\qquad\overset{(6.38b)}{=}\Sigma\pmb{A}^{\top}\,,}\end{array}
$$  

其中 $\pmb{\Sigma}=\mathbb{E}[\pmb{x x}^{\top}]-\pmb{\mu}\pmb{\mu}^{\top}$ 是 $X$ 的协方差。

### 示例 6.5  

考虑一个具有零均值（$\mathbb{E}_{X}[x]\;=\;0$）的随机变量 $X$ ，以及 $\mathbb{E}_{X}[x^{3}]=0$ 。令 $y=x^{2}$ （因此，$Y$ 依赖于 $X$ ），并考虑 $X$ 和 $Y$ 之间的协方差（6.36）。但这给出  

$$
\mathrm{Cov}[x,y]=\mathbb{E}[x y]-\mathbb{E}[x]\mathbb{E}[y]=\mathbb{E}[x^{3}]=0\,。
$$  

在机器学习中，我们通常考虑可以建模为 独立同分布（i.i.d.） 的随机变量的问题，$X_{1},\dots,X_{N}$ 。对于超过两个随机变量的情况，通常所说的“独立”（定义6.10）指的是相互独立的随机变量，其中所有子集都是独立的（参见Pollard (2002, chapter 4) 和 Jacod and Protter (2004, chapter 3)）。短语“独立同分布”意味着所有随机变量都来自同一分布。  

机器学习中另一个重要的概念是条件独立性。  

### 定义6.11（条件独立性） 。

两个随机变量 $X$ 和 $Y$ 给定 $Z$ 条件下独立，当且仅当  

$$
p(\pmb{x},\pmb{y}\,|\,z)=p(\pmb{x}\,|\,z)p(\pmb{y}\,|\,z)\quad\mathrm{for\,all}\quad z\in\mathcal{Z}\,，
$$  

其中 $\mathcal{Z}$ 是随机变量 $Z$ 的状态集。我们写作 $X\perp\!\perp Y\,|\,Z$ ⊥  | 表示 $X$ 条件下与 $Y$ 独立于 $Z$ 。  

定义6.11要求在（6.55）中的关系必须对所有 $z$ 的值都成立。等式（6.55）的解释可以理解为“给定关于 $z$ 的知识，$x$ 和 $\pmb{ y}$ 的分布可以分解”。独立性可以作为条件独立性的特殊情况来表示，如果我们写作 $X$ ⊥ $Y\mid\emptyset$ 。通过使用概率的乘法规则（6.22），我们可以将（6.55）的左侧展开为  

$$
\begin{array}{r}{p(\pmb{x},\pmb{y}\,|\,z)=p(\pmb{x}\,|\,\pmb{y},z)p(\pmb{y}\,|\,z)\,.}\end{array}
$$  

通过比较（6.55）的右侧与（6.56），我们看到 $p(\pmb{y}\,|\,\pmb{z})$ 出现在两者中，因此  

$$
p(\pmb{x}\,|\,\pmb{y},z)=p(\pmb{x}\,|\,z)\,。
$$  

等式（6.57）提供了条件独立性的另一种定义，即 $\textit{X}\perp\!\perp\textit{Y}\,|\,Z$ ⊥  | 。这种替代展示提供了解释“给定我们知道了 $z$ ，关于 $\pmb{ y}$ 的知识不会改变我们对 $\pmb{x}^{\prime}$ 的知识”。


### 6.4.6 随机变量的内积

回想一下第3.2节中对内积的定义。我们可以定义随机变量之间的内积，这部分内容我们在本节中简要描述。如果两个不相关随机变量 $X,Y$ ，则

$$
\mathbb{V}[x+y]=\mathbb{V}[x]+\mathbb{V}[y]\,。
$$  

多元随机变量之间的内积可以以类似的方式处理  

由于方差的度量单位是平方单位，这看起来非常像直角三角形的毕达哥拉斯定理 $c^{2}=a^{2}+b^{2}$ 。  

接下来，我们看看是否可以找到不相关随机变量方差关系（6.58）的几何解释。  

![](images/3c9ee252094d6f2fd8dec2a476802f513e1fbba879d1bb025c299c8717e5c78c.jpg)  
图6.6 随机变量的几何。如果随机变量 $X$ 和 $Y$ 不相关，它们在相应的向量空间中是正交向量，并且毕达哥拉斯定理适用。  

随机变量可以被视为向量空间中的向量，并且我们可以定义内积以获得随机变量的几何性质（Eaton, 2007）。如果我们定义  

$$
\langle X,Y\rangle:=\mathrm{Cov}[x,y]
$$  

$$
\begin{array}{r l}&{\mathrm{Cov}[x,x]=0\iff}\\ &{x=0}\\ &{\mathrm{Cov}[\alpha x+z,y]=}\\ &{\alpha\,\mathrm{Cov}[x,y]+}\end{array}
$$  

对于零均值随机变量 $X$ 和 $Y$ ，我们得到一个内积。我们看到协方差是对称的、正定的，并且在任一参数上都是线性的。随机变量的长度是  

$$
\|X\|=\sqrt{\mathrm{Cov}[{\boldsymbol x},{\boldsymbol x}]}=\sqrt{\mathbb{V}[{\boldsymbol x}]}=\sigma[{\boldsymbol x}]\,，
$$ 

即它的标准差。随机变量“更长”意味着它越不确定；长度为 0 的随机变量是确定的。如果我们考虑两个随机变量 $X,Y$ 之间的角度 $\theta$ ，我们得到  

$$
\cos\theta=\frac{\langle X,Y\rangle}{\|X\|\,\|Y\|}=\frac{\operatorname{Cov}[x,y]}{\sqrt{\mathbb{V}[x]\mathbb{V}[y]}}\,，
$$  

这是两个随机变量之间的相关性（定义6.8）。这意味着当我们从几何角度考虑随机变量时，我们可以将相关性视为两个随机变量之间的余弦。我们知道从  finiti  3.7 中得知 $X\bot Y\iff\langle X,Y\rangle=0.$ ⇐ ⇒⟨  ⟩ 。在我们的情况下，这意味着 $X$ 和 $Y$ 正交当且仅当 $\mathrm{Cov}[x,y]=0$ ，即它们是不相关的。图6.6 描述了这种关系。  

注释。 尽管使用从先前内积定义构建的欧几里得距离来比较概率分布很有诱惑力，但不幸的是，这并不是获得分布之间距离的最佳方式。请记住，概率质量（或密度）是正的，并且需要加起来等于1。这些约束意味着分布生活在被称为统计流形的东西上。研究这种概率分布空间的几何学称为信息几何学。计算分布之间的距离通常使用库尔伯格-莱布里尔距离，这是考虑到统计流形性质的一般距离类别的特例。就像欧几里得距离是度量（第3.3节）的特殊案例一样，库尔伯格-莱布里尔距离是两类更一般的距离类别的特例，即Bregman距离和$f$ -距离。关于距离的研究超出了本书的范围，我们参考Amari (2016)这本书，它是信息几何学领域创始人之一的详细资料。

## 6.5 高斯分布  

高斯分布是连续值随机变量最广泛研究的概率分布。它也被称作 正态分布 。它的重要性源于它具有许多计算上方便的性质，我们将在下面讨论。特别是，我们将使用它来定义线性回归的似然函数和先验概率（第9章），并考虑高斯混合模型用于密度估计（第11章）。  

在机器学习的许多其他领域，也受益于使用高斯分布，例如高斯过程、变分推断和强化学习。它在信号处理（例如卡尔曼滤波器）、控制（例如线性二次调节）和统计学（例如假设检验）等其他应用领域中也广泛使用。  

正态分布 当我们考虑独立且同分布随机变量之和时，高斯分布自然出现。这被称为中心极限定理（Grinstead and Snell, 1997）。  

![](images/2ba218f57a9b1455b918f1d24292fb061a530998dc5614a66324e027bcf18ba3.jpg)  
(a) 单变量（一维）高斯分布；红色十字表示均值，红色线表示方差的范围。  

![](images/5dc31318be2ee2b0dd41de9ec1d6272fcce2e3101a507c6658f37ec8f26dfbb3.jpg)  
(b) 多变量（二维）高斯分布，从顶部观察。红色十字表示均值，彩色线条表示密度的等值线。  

对于单变量随机变量，高斯分布的密度由以下给出  

$$
p(x\,|\,\mu,\sigma^{2})={\frac{1}{\sqrt{2\pi\sigma^{2}}}}\exp\left(-{\frac{(x-\mu)^{2}}{2\sigma^{2}}}\right).
$$  

多变量高斯分布 由均值向量 $\pmb{\mu}$ 和协方差矩阵 $\pmb{\Sigma}$ 完全描述，并定义为  

$$
p(\pmb{x}\,|\,\pmb{\mu},\pmb{\Sigma})=(2\pi)^{-\frac{D}{2}}|\pmb{\Sigma}|^{-\frac{1}{2}}\exp\big(-\textstyle\frac{1}{2}(\pmb{x}-\pmb{\mu})^{\top}\pmb{\Sigma}^{-1}(\pmb{x}-\pmb{\mu})\big)\,，
$$  

其中 $\pmb{x}\,\in\,\mathbb{R}^{D}$ 。我们写作 $p(\pmb{x})\,=\,\mathcal{N}(\pmb{x}\,|\,\pmb{\mu},\,\pmb{\Sigma})$ 或 $X\,\sim{\mathcal{N}}({\pmb{\mu}},\,{\pmb{\Sigma}})$ 。图6.7显示了二维高斯分布（网格），以及相应的等值线图。图6.8显示了单变量高斯分布和二维高斯分布及其对应的样本。具有零均值和单位协方差的特殊情况，即 $\pmb{\mu}=\mathbf{0}$ 和 $\pmb{\Sigma}=\pmb{I}$ ，被称为 标准正态分布 。  

高斯分布广泛用于统计估计和机器学习，因为它们具有边缘分布和条件分布的封闭形式表达式。在第9章中，我们广泛使用这些封闭形式表达式进行线性回归。使用高斯随机变量建模的一个主要优点是通常不需要变量转换（第6.7节）。由于高斯分布完全由其均值和协方差确定，我们经常可以通过将转换应用于随机变量的均值和协方差来获得转换后的分布。



### 6.5.1 高斯分布的边缘分布和条件分布  

以下，我们将在多变量随机变量的一般情况下展示边缘分布和条件分布。如果初次阅读时感到困惑，建议读者考虑两个单变量随机变量。令 $X$ 和 $Y$ 是两个可能具有不同维度的多变量随机变量。为了考虑应用概率求和规则和条件的影响，我们明确地将高斯分布表示为连接状态 $[{\pmb{x}}^{\top}\ {\pmb y}^{\top}]^{\top}$ 的形式，使得  

$$
p(\pmb{x},\pmb{y})=\mathcal{N}\left(\left[\pmb{\mu}_{x}\right],\,\left[\pmb{\Sigma}_{x x}\quad\pmb{\Sigma}_{x y}\right]\right)\,,
$$

其中 $\pmb{\Sigma}_{x x}\,=\,\mathrm{Cov}[\pmb{x},\pmb{x}]$ 和 $\pmb{\Sigma}_{y y}\,=\,\mathrm{Cov}[\pmb{y},\pmb{y}]$ 分别是 $x$ 和 $\pmb{ y}$ 的边缘协方差矩阵，而 $\pmb{\Sigma}_{x y}=\mathrm{Cov}[\pmb{x},\pmb{y}]$ 是 $x$ 和 $\pmb{ y}$ 之间的交叉协方差矩阵。  

条件分布 $p(\pmb{x}\,|\,\pmb{y})$ 也是高斯分布（如图6.9(c)所示），并由（在Bishop, 2006的第2.3节中推导）给出  

$$
\begin{array}{r l}&{p(\pmb{x}\,|\,\pmb{y})=\mathcal{N}\big(\pmb{\mu}_{x\,|\,y},\,\pmb{\Sigma}_{x\,|\,y}\big)}\\ &{\quad\pmb{\mu}_{x\,|\,y}=\pmb{\mu}_{x}+\pmb{\Sigma}_{x y}\pmb{\Sigma}_{y y}^{-1}(\pmb{y}-\pmb{\mu}_{y})}\\ &{\quad\pmb{\Sigma}_{x\,|\,y}=\pmb{\Sigma}_{x x}-\pmb{\Sigma}_{x y}\pmb{\Sigma}_{y y}^{-1}\pmb{\Sigma}_{y x}\,.}\end{array}
$$

请注意，在计算（6.66）中的均值时，$\pmb{ y}$ 的值是一个观察结果，不再是随机的。  

注释。 条件高斯分布出现在许多地方，我们对后验分布感兴趣：  

卡尔曼滤波器（Kalman, 1960），信号处理中最核心的算法之一，所做的只是计算联合分布的高斯条件分布（Deisenroth and Ohlsson, 2011; S¨ arkk¨ a, 2013）。高斯过程（Rasmussen and Williams, 2006），这是函数分布的实用实现。在高斯过程中，我们假设随机变量的联合高斯性。通过（高斯）条件化观察数据，我们可以确定函数的后验分布。潜在线性高斯模型（Roweis and Ghahramani, 1999; Mur- phy, 2012），包括概率主成分分析（PPCA）（Tipping and Bishop, 1999）。我们将更详细地在第10.7节中探讨PPCA。  

联合高斯分布的边缘分布 $p(\pmb{x})$ （参见（6.64））本身是高斯分布，并通过应用求和规则（6.20）计算得出，给出为  

$$
p(\pmb{x})=\int p(\pmb{x},\pmb{y})\mathrm{d}\pmb{y}=\mathcal{N}\big(\pmb{x}\,|\,\pmb{\mu}_{x},\,\pmb{\Sigma}_{x x}\big)\,。
$$  

对于 $p(\pmb{y})$ 的相应结果也成立，这是通过关于 $\pmb{x}$ 的边缘化获得的。直观地看，考虑联合分布（6.64），我们忽略（即，积分出去）我们不感兴趣的任何内容。这在图6.9(b)中进行了说明。  

![](images/300b917604368f454dcf987458f39828bbe514ef2fac1a4c68b7aaed34958bf7.jpg)  

考虑二维高斯分布（如图6.9所示）：  

$$
p(x_{1},x_{2})=\mathcal{N}\left(\left[\begin{array}{c}{0}\\ {2}\end{array}\right],\,\left[\begin{array}{c c}{0.3}&{-1}\\ {-1}&{5}\end{array}\right]\right)\,。
$$  

通过应用（6.66）和（6.67），我们可以计算在 $x_{2}=-1$ 条件下的单变量高斯的参数，分别得到均值和方差。数值上，这是  

$$
\mu_{x_{1}\,|\,x_{2}=-1}=0+(-1)\cdot0.2\cdot(-1-2)=0.6
$$  

和  

$$
\sigma_{x_{1}\,|\,x_{2}=-1}^{2}=0.3-(-1)\cdot0.2\cdot(-1)=0.1\,。
$$ 

因此，条件高斯分布给出为  

$$
p(x_{1}\,|\,x_{2}=-1)=\mathcal{N}\big(0.6,\,0.1\big)\,。
$$  

相比之下，边缘分布 $p(x_{1})$ 可以通过应用（6.68）获得，这基本上使用了随机变量 $x_{1}$ 的均值和方差，给出我们  

$$
\begin{array}{r}{p(x_{1})=\mathcal{N}\big(0,\,0.3\big)\,.}\end{array}
$$




### 6.5.2 高斯密度的乘积  

对于线性回归（第9章），我们需要计算高斯似然性。此外，我们可能希望假设高斯先验（第9.3节）。我们应用贝叶斯定理来计算后验，这导致了似然性和先验的乘积，即两个高斯密度的乘积。两个高斯分布 $\mathcal{N}(\boldsymbol{\mathscr{x}}\,|\,\boldsymbol{a},\,\bar{\boldsymbol{A}})\mathcal{N}(\boldsymbol{\mathscr{x}}\,|\,\boldsymbol{b},\,\boldsymbol{B})$ 的 乘积 是一个由 $c\in\mathbb{R}$ 缩放的高斯分布，给出为 $c\mathcal{N}(\pmb{x}\,|\,c,\,C)$，其中  

$$
\begin{array}{r l}&{C=(A^{-1}+B^{-1})^{-1}}\\ &{c=C(A^{-1}a+B^{-1}b)}\\ &{c=(2\pi)^{-\frac{D}{2}}|A+B|^{-\frac{1}{2}}\exp\big(-\frac{1}{2}(a-b)^{\top}(A+B)^{-1}(a-b)\big)\,.}\end{array}
$$
缩放常数 $c$ 本身可以以 $\pmb{ a}$ 或 $^{b}$ 的形式写成高斯密度，带有“膨胀”的协方差矩阵 $A+B$，即 $c=\mathcal{N}\big(a\,|\,b,\,A+B\big)=\mathcal{N}\big(b\,|\,a,\,A+B\big)$。  

注释。 为了方便记号，有时我们会使用 $\mathcal{N}(\pmb{x}\,|\,\pmb{m},\,\pmb{S})$ 来描述高斯密度的函数形式，即使 $x$ 不是一个随机变量。我们在前面的演示中就是这样做的，当我们写  

$$
c=\textstyle{\mathcal{N}}\big(a\,|\,b,\,A+B\big)=\textstyle{\mathcal{N}}\big(b\,|\,a,\,A+B\big)\,。
$$
这里，$\pmb{ a}$ 和 $^{b}$ 都不是随机变量。然而，以这种方式写出 $c$ 比（6.76）更紧凑。$\diamondsuit$




### 6.5.3 和与线性变换  

如果 $X,Y$ 是独立的高斯随机变量（即联合分布给出为 $p(\pmb{x},\pmb{y})\,=\,p(\pmb{x})p(\pmb{y}))$，其中 $p(\pmb{x})=\mathcal{N}\big(\pmb{x}\,|\,\pmb{\mu}_{x},\,\pmb{\Sigma}_{x}\big)$ 和 $p(\pmb{y})=\mathcal{N}\big(\pmb{y}\,|\,\pmb{\mu}_{y},\,\pmb{\Sigma}_{y}\big)$，那么 $\ {\pmb{x}}+{\pmb{ y}}$ 也是高斯分布，并给出为  

$$
p(\pmb{x}+\pmb{y})=\mathcal{N}\big(\pmb{\mu}_{x}+\pmb{\mu}_{y},\,\pmb{\Sigma}_{x}+\pmb{\Sigma}_{y}\big)\:。
$$
知道 $p(\pmb{x}+\pmb{y})$ 是高斯分布，可以直接使用（6.46）至（6.49）的结果来确定均值和协方差矩阵。当考虑独立同分布的高斯噪声作用于随机变量时，例如在线性回归（第9章）中，这个性质将变得非常重要。




### 示例 6.7  

由于期望是线性操作，我们可以获得独立高斯随机变量的加权和  

$$
p(a{\pmb{x}}+b{\pmb y})=\mathcal{N}\big(a{\pmb\mu}_{x}+b{\pmb\mu}_{y},\,a^{2}{\pmb\Sigma}_{x}+b^{2}{\pmb\Sigma}_{y}\big)\:。
$$
注释。 在第11章中将有用的一个案例是高斯密度的加权和。这与高斯随机变量的加权和不同。$\diamondsuit$  

在定理 6.12 中，随机变量 $x$ 来自一个密度，该密度是两个密度 $p_{1}(x)$ 和 $p_{2}(x)$ 的混合，加权系数为 $\alpha$ 。由于期望的线性性也适用于多变量随机变量，因此定理可以推广到多变量随机变量的情况。然而，平方随机变量的概念需要被替换为 $\mathbf{\Delta}\mathbf{\mathit{x}}\mathbf{\delta}^{\top}$。  

定理 6.12。 考虑两个单变量高斯密度的混合  

$$
p(x)=\alpha p_{1}(x)+(1-\alpha)p_{2}(x)\:，
$$
其中标量 $0<\alpha<1$ 是混合权重，而 $p_{1}(x)$ 和 $p_{2}(x)$ 是具有不同参数的单变量高斯密度（等式 (6.62)），即 $(\mu_{1},\sigma_{1}^{2})\neq(\mu_{2},\sigma_{2}^{2})$。  

则混合密度 $p(x)$ 的均值由每个随机变量的均值的加权和给出：  

$$
\begin{array}{r}{\mathbb{E}[x]=\alpha\mu_{1}+(1-\alpha)\mu_{2}\:。}\end{array}
$$
混合密度 $p(x)$ 的方差给出为  

$$
\mathbb{V}[x]=\left[\alpha\sigma_{1}^{2}+(1-\alpha)\sigma_{2}^{2}\right]+\left(\left[\alpha\mu_{1}^{2}+(1-\alpha)\mu_{2}^{2}\right]-\left[\alpha\mu_{1}+(1-\alpha)\mu_{2}\right]^{2}\right)\:。
$$
证明。 混合密度 $p(x)$ 的均值由每个随机变量的均值的加权和给出。我们应用均值的定义（定义 6.4），并插入我们的混合（6.80），得到  

$$
{\begin{array}{r l}&{\mathbb{E}[x]=\displaystyle\int_{-\infty}^{\infty}x p(x)\mathrm{d}x}\\ &{\qquad=\displaystyle\int_{-\infty}^{\infty}\left(\alpha x p_{1}(x)+(1-\alpha)x p_{2}(x)\right)\mathrm{d}x}\\ &{\qquad=\alpha\displaystyle\int_{-\infty}^{\infty}x p_{1}(x)\mathrm{d}x+(1-\alpha)\int_{-\infty}^{\infty}x p_{2}(x)\mathrm{d}x}\\ &{\qquad=\alpha\mu_{1}+(1-\alpha)\mu_{2}\:。}\end{array}}
$$
为了计算方差，我们可以使用（6.44）中的原始分数版本的方差，这需要平方随机变量的期望的表达式。在这里，我们使用随机变量函数期望的定义（定义 6.3），




$$
\begin{array}{l}{\displaystyle\mathbb{E}[x^{2}]=\int_{-\infty}^{\infty}x^{2}p(x)\mathrm{d}x}\\ {\displaystyle\qquad=\int_{-\infty}^{\infty}\left(\alpha x^{2}p_{1}(x)+(1-\alpha)x^{2}p_{2}(x)\right)\mathrm{d}x}\end{array}
$$
$
\begin{array}{r l}
 &{{}=\alpha\int_{-\infty}^{\infty}x^{2}p_{1}(x)\mathrm{d}x+(1-\alpha)\int_{-\infty}^{\infty}x^{2}p_{2}(x)\mathrm{d}x}\\
{\ }&{{}=\alpha(\mu_{1}^{2}+\sigma_{1}^{2})+(1-\alpha)(\mu_{2}^{2}+\sigma_{2}^{2})\,}
\end{array}
$  

在最后的等式中，我们再次使用了方差的原始分数版本（6.44），给出 $\sigma^{2}=\mathbb{E}[x^{2}]-\mu^{2}$。这个等式被重新排列为平方的期望随机变量是平方的均值和方差的和。  

因此，方差由（6.83d）减去（6.84d）给出，  

$$
\begin{array}{r l}&{\mathbb{V}[x]=\mathbb{E}[x^{2}]-(\mathbb{E}[x])^{2}}\\ &{\qquad=\alpha(\mu_{1}^{2}+\sigma_{1}^{2})+(1-\alpha)(\mu_{2}^{2}+\sigma_{2}^{2})-(\alpha\mu_{1}+(1-\alpha)\mu_{2})^{2}}\\ &{\qquad=\left[\alpha\sigma_{1}^{2}+(1-\alpha)\sigma_{2}^{2}\right]}\\ &{\qquad+\left(\left[\alpha\mu_{1}^{2}+(1-\alpha)\mu_{2}^{2}\right]-[\alpha\mu_{1}+(1-\alpha)\mu_{2}]^{2}\right)\,.}\end{array}
$$
注释。 上述推导适用于任何密度，但由于高斯分布完全由均值和方差确定，混合密度可以以闭式形式确定。$\diamondsuit$  

对于混合密度，各个组件可以被视为条件分布（条件于组件身份）。等式（6.85c）是条件方差公式的例子，也被称为 总方差法则 ，通常表示为对于两个随机变量 $X$ 和 $Y$，$\mathbb{V}_{X}[x]=\mathbb{E}_{Y}[\mathbb{V}_{X}[x|y]]+\mathbb{V}_{Y}[\mathbb{E}_{X}[x|y]]$，即随机变量的（总）方差是条件方差的期望加上条件均值的方差。  

总方差法则  

我们考虑在示例 6.17 中的双变量标准高斯随机变量 $X$ 并对其执行线性变换 $_{A x}$。结果是一个均值为零且协方差为 $A A^{\top}$ 的高斯随机变量。注意，添加一个常数向量会改变分布的均值，而不影响其方差，也就是说，随机变量 ${\pmb{x}}+{\pmb\mu}$ 是均值为 $\pmb{\mu}$ 且协方差为单位矩阵的高斯分布。因此，任何线性/仿射变换的高斯随机变量都是高斯分布的。  

任何线性/仿射变换的高斯随机变量也是高斯分布的。  

考虑一个 高斯分布的随机变量 $X\sim{\mathcal{N}}(\mu,\,\Sigma)$。对于给定的矩阵 $A$ 的适当形状，让 $Y$ 是一个随机变量，使得 $\pmb{y}=\pmb{A}\pmb{x}$ 是 $\pmb{x}$ 的变换版本。我们可以利用期望是线性操作（6.50）来计算 $\pmb{y}$ 的均值：  

$$
\operatorname{E}[{\pmb y}]=\operatorname{E}[A{\pmb{x}}]=A\operatorname{\mathbb{E}}[{\pmb{x}}]=A{\pmb\mu}\,。
$$
同样，$\pmb{y}$ 的方差可以通过使用（6.51）找到：  

$$
\mathbb{V}[\pmb{y}]=\mathbb{V}[\pmb{A}\pmb{x}]=\pmb{A}\mathbb{V}[\pmb{x}]\pmb{A}^{\top}=\pmb{A}\pmb{\Sigma}\pmb{A}^{\top}\,。
$$
这意味着随机变量 $\pmb{y}$ 的分布遵循  

$$
p(\pmb{y})=\mathcal{N}\big(\pmb{y}\,|\,\pmb{A}\pmb{\mu},\,\pmb{A}\pmb{\Sigma}\pmb{A}^{\top}\big)。
$$
现在让我们考虑逆向变换：当我们知道一个随机变量的均值是另一个变量的线性变换时。对于给定的满秩矩阵 $\pmb{A}\in\mathbb{R}^{M\times N}$，其中 $M\geqslant N$，让 $\pmb{y}\in\mathbb{R}^{M}$ 是一个高斯随机变量，其均值为 $Ax$，即  

$$
p(\pmb{y})=\mathcal{N}\big(\pmb{y}\,|\,\pmb{A}\pmb{x},\,\pmb{\Sigma}\big)\,。
$$
对应的概率分布 $p(\pmb{x})$ 是什么？如果 $\pmb{A}$ 可逆，则我们可以写出 $\bar{\pmb{x}}\overset{=}{=}\pmb{A}^{-1}\pmb{y}$ 并应用上一段中的变换。然而，通常 $\pmb{A}$ 不可逆，我们使用类似于伪逆（3.57）的方法。也就是说，我们先乘以 $\pmb{A}^{\top}$，然后求解 $\pmb{A}^{\top}\pmb{A}$ 的逆，$\pmb{A}^{\top}\pmb{A}$ 是对称的且正定的，给出我们之间的关系  

$$
\pmb{y}=\pmb{A}\pmb{x}\iff(\pmb{A}^{\top}\pmb{A})^{-1}\pmb{A}^{\top}\pmb{y}=\pmb{x}\,。
$$
因此，$\pmb{x}$ 是 $\pmb{y}$ 的线性变换，我们得到  

$$
p(\pmb{x})=\mathcal{N}\big(\pmb{x}\,|\,(\pmb{A}^{\top}\pmb{A})^{-1}\pmb{A}^{\top}\pmb{y},\,(\pmb{A}^{\top}\pmb{A})^{-1}\pmb{A}^{\top}\pmb{\Sigma}\pmb{A}(\pmb{A}^{\top}\pmb{A})^{-1}\big)\,。
$$
### 6.5.4 从多变量高斯分布中采样  

我们不会详细解释计算机上的随机采样细节，对此感兴趣的读者可以参考 Gentle (2004)。对于多变量高斯分布，这个过程包括三个阶段：首先，我们需要一个提供 [0,1] 区间内均匀样本的伪随机数源；其次，我们使用非线性变换，如 Box-Muller 变换（Devroye, 1986）来获得单变量高斯分布的样本；最后，我们将这些样本组合成一个向量，以获得来自标准多变量正态分布 $\mathcal{N}(\mathbf{0},\,I)$ 的样本。  

为了计算矩阵的 Cholesky 分解，要求矩阵是对称的且正定的（第 3.2.3 节）。协方差矩阵具有这种性质。  

对于一般的多变量高斯分布，即均值非零且协方差矩阵不是单位矩阵的情况下，我们利用高斯随机变量线性变换的性质。假设我们感兴趣的是生成样本 $\pmb{x}_{i},i=1,\ldots,n$，来自具有均值 $\pmb{\mu}$ 和协方差矩阵 $\pmb{\Sigma}$ 的多变量高斯分布。我们希望从提供标准多变量正态分布 $\mathcal{N}(\mathbf{0},\,I)$ 的样本的采样器中构建样本。  

为了从多变量正态分布 $\mathcal{N}(\pmb{\mu},\,\pmb{\Sigma})$ 中获得样本，我们可以利用高斯随机变量线性变换的性质：如果 $\pmb{x}\,\sim\mathcal{N}(\mathbf{0},\,\pmb{I})$，则 ${\pmb y}={\pmb A}{\pmb{x}}+{\pmb\mu}$，其中 $\mathbf{A}\mathbf{A}^{\intercal}=\mathbf{\Sigma}$ 是均值为 $\pmb{\mu}$ 且协方差矩阵为 $\pmb{\Sigma}$ 的高斯分布。一个方便的选择是使用协方差矩阵 $\pmb{\Sigma}=\pmb{A}\pmb{A}^{\top}$ 的 Cholesky 分解（第 4.3 节）。Cholesky 分解的好处是 $\pmb{A}$ 是三角形的，这导致了高效的计算。




## 6.6 结合性和指数族  

在统计教科书中找到的“有名”的概率分布大多被发现用于描述特定类型的现象。例如，我们在第 6.5 节中看到了高斯分布。这些分布之间也以复杂的方式相互关联（Leemis 和 McQueston, 2008）。对于该领域的初学者来说，确定使用哪种分布可能会感到困惑。此外，许多这些分布是在统计和计算仅通过铅笔和纸进行的时代发现的。在计算时代，提出有意义的概念是自然的（Efron 和 Hastie, 2016）。在上一节中，我们看到许多用于推断的操作在分布为高斯时可以方便地计算。此时值得回忆的是，在机器学习上下文中操作概率分布时的期望目标：

1. 应用概率规则时存在某种“封闭性质”，例如贝叶斯定理。通过封闭，我们意味着应用特定的操作会返回相同类型的对象。

2. 随着数据的积累，我们不需要更多的参数来描述分布。

3. 由于我们对从数据中学习感兴趣，我们希望参数估计行为良好。

结果发现，被称为指数族的分布类提供了适当的通用性，同时保持了有利的计算和推断特性。在介绍指数族之前，让我们看看三个“有名”概率分布的成员：伯努利分布（示例 6.8）、二项分布（示例 6.9）和贝塔分布（示例 6.10）。




### 示例 6.8  

伯努利分布 是一个针对单一二进制随机变量 $X$ 的分布，其状态为 $x\in\{0,1\}$。它由一个连续参数 $\mu\in[0,1]$ 管理，该参数代表了 $X=1$ 的概率。伯努利分布 $\mathrm{Ber}(\mu)$ 定义为  

$$
\begin{array}{r l}&{p(x\,|\,\mu)=\mu^{x}(1-\mu)^{1-x}\,,\quad x\in\{0,1\}\,,}\\ &{\quad\mathbb{E}[x]=\mu\,,}\\ &{\quad\mathbb{V}[x]=\mu(1-\mu)\,,}\end{array}
$$
其中 $\mathbb{E}[x]$ 和 $\mathbb{V}[x]$ 分别是二进制随机变量 $X$ 的均值和方差。  

指数族  

伯努利分布  

![](images/d741492e9d5c870e36c277470d4e10af26f957a611ca948e24f3187055c1ad7d.jpg)  

伯努利分布的一个应用示例是当我们对掷硬币时“正面”出现的概率感兴趣时。  

![](images/6e1834582655b6605d35f2baf664ee5bfa64eb7cba7b54926486ab1ee8ceca4a.jpg)  
图 6.10 当 $\mu\in\{0.1,0.4,0.75\}$ 且 $N=15$ 时的二项分布示例。  

注释。 上述对伯努利分布的重写，其中我们使用布尔变量作为数值 0 或 1 并在指数中表达它们，是机器学习教科书中经常使用的小技巧。在表示多项式分布时也会出现这种情况。$\diamondsuit$




### 示例 6.9（二项分布）  

二项分布 二项分布是对伯努利分布的推广，用于描述整数分布（如图 6.10 所示）。具体而言，二项分布可以用来描述从伯努利分布中抽取 $N$ 个样本时观察到 $m$ 次 $X=1$ 的概率，其中伯努利分布的参数 $p(X\,=\,1)\,=\,\mu\,\in\,[0,1]$。二项分布 $\mathrm{Bin}(N,\mu)$ 定义为  

$$
\begin{array}{c}{{p(m\,|\,N,\mu)=\binom N m\mu^{m}(1-\mu)^{N-m}\,,}}\\ {{\mathbb E[m]=N\mu\,,}}\\ {{\mathbb V[m]=N\mu(1-\mu)\,,}}\end{array}
$$
其中 $\mathbb{E}[m]$ 和 $\mathbb{V}[m]$ 分别是 $m$ 的均值和方差。  

如果我们要描述在单次实验中观察到 $m$ 次“正面”在 $N$ 次硬币抛掷实验中的概率，且单次实验观察到正面的概率为 $\mu$，则二项分布可以被使用。

### 示例 6.10（贝塔分布）  

贝塔分布 我们可能希望对有限区间内的连续随机变量进行建模。贝塔分布是对连续随机变量 $\mu\in[0,1]$ 的分布，经常用于表示某些二进制事件的概率（例如，伯努利分布的参数）。贝塔分布 $\mathtt{B e t a}(\alpha,\beta)$（如图 6.11 所示）由两个参数 $\alpha>0,\;\beta>0$ 管理  

$$
\begin{array}{c}{{p(\mu\,|\,\alpha,\beta)=\displaystyle\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\mu^{\alpha-1}(1-\mu)^{\beta-1}}}\\ {{\mathbb{E}[\mu]=\displaystyle\frac{\alpha}{\alpha+\beta}\,,\qquad\mathbb{V}[\mu]=\frac{\alpha\beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)}}}\end{array}
$$
其中 $\Gamma(\cdot)$ 是伽玛函数，定义为  

$$
\begin{array}{l}{\displaystyle\Gamma(t):=\int_{0}^{\infty}x^{t-1}\exp(-x)d x,\qquad t>0\,.}\\ {\displaystyle\Gamma(t+1)=t\Gamma(t)\,.}\end{array}
$$
注意，式（6.98）中伽玛函数的分数用于归一化贝塔分布。  

![](images/0b9d7922a600165d838ce53652be97d7eba8a560c3fe8f268e1c1ea27e505f69.jpg)  
图 6.11 不同 $\alpha$ 和 $\beta$ 值下的贝塔分布示例。  

直观上，$\alpha$ 将概率质量推向 1，而 $\beta$ 将概率质量推向 0。有一些特殊情况（Murphy, 2012）：  

对于 $\alpha=1=\beta$，我们得到均匀分布 $\mathcal{U}[0,1]$。对于 $\alpha,\beta<1$，我们得到具有 0 和 1 两个峰值的双峰分布。对于 $\alpha,\beta>1$，分布为单峰。对于 $\alpha,\beta>1$ 且 $\alpha=\beta$，分布为单峰、对称且位于区间 $[0,1]$ 中心，即模式/均值为 $\frac{1}{2}$。  

注释。 有许多带有名称的分布，它们以不同的方式相互关联（Leemis 和 McQueston, 2008）。值得记住的是，每个命名的分布都有其特定的创建原因，但可能有其他应用。了解特定分布创建背后的原因往往能提供如何最佳使用它的洞察。我们引入了前面三个分布，以便能够说明结合性（第 6.6.1 节）和指数族（第 6.6.3 节）的概念。$\diamondsuit$



### 6.6.1 结合性  

根据贝叶斯定理（6.23），后验分布与先验分布和似然函数的乘积成比例。先验分布的指定可能会有困难，原因有两个：首先，先验分布应该包含我们在看到任何数据之前对问题的了解。这通常很难描述。其次，通常无法通过分析计算后验分布。然而，有一些计算上方便的先验分布：结合先验。  

定义 6.13（结合先验）。如果后验分布与似然函数的形式/类型相同，则先验分布为结合先验。  

结合性特别方便，因为我们可以通过更新先验分布的参数来代数计算我们的后验分布。  

注释。 当考虑概率分布的几何结构时，结合先验保留了与似然函数相同的距离结构（Agarwal 和 Daum´ e III, 2010）。$\diamondsuit$  

为了引入结合先验的具体示例，我们在示例 6.11 中描述了二项分布（定义在离散随机变量上）和贝塔分布（定义在连续随机变量上）。



### 示例 6.11（Beta-Binomial 结合性）  

考虑一个二项随机变量 $x\sim\mathrm{Bin}(N,\mu)$，其中  

$$
p(x\,|\,N,\mu)=\binom N x\mu^{x}(1-\mu)^{N-x}\,,\quad x=0,1,\cdots,N\,,
$$
是找到 $x$ 次“正面”结果在 $N$ 次硬币抛掷中的概率，其中 $\mu$ 是“正面”的概率。我们对参数 $\mu$ 使用贝塔先验，即 $\mu\sim\mathsf{B e t a}(\alpha,\beta)$，其中  

$$
p(\mu\,|\,\alpha,\beta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\mu^{\alpha-1}(1-\mu)^{\beta-1}\,.
$$
现在如果我们观察到一些结果 $x=h$，即在 $N$ 次硬币抛掷中看到 $h$ 次“正面”，我们计算 $\mu$ 的后验分布为  

$$
\begin{array}{r l}&{p(\mu\,|\,x=h,N,\alpha,\beta)\propto p(x\,|\,N,\mu)p(\mu\,|\,\alpha,\beta)}\\ &{\phantom{\mu\,x=h}\propto\mu^{h}(1-\mu)^{(N-h)}\mu^{\alpha-1}(1-\mu)^{\beta-1}}\\ &{\phantom{\mu\,x=h}=\mu^{h+\alpha-1}(1-\mu)^{(N-h)+\beta-1}}\end{array}
$$
$$
\propto\mathrm{Beta}(h+\alpha,N-h+\beta)\,,
$$
即后验分布是与先验相同的Beta分布，即贝塔先验是二项似然函数参数 $\mu$ 的结合先验。  

在以下示例中，我们将推导一个类似于Beta-Binomial结合性结果的结果。在这里，我们将展示贝塔分布是伯努利分布的结合先验。  

### 示例 6.12（Beta-Bernoulli 结合性）  

令 $x\in\{0,1\}$ 服从参数为 $\theta\in[0,1]$ 的伯努利分布，即 $p(x=1\,|\,\theta)=\theta$。这也可以表示为 $p(x\,|\,\theta)=\theta^{x}(1-\theta)^{1-x}$。令 $\theta$ 服从参数为 $\alpha,\beta$ 的分布，即 $p(\theta\,|\,\alpha,\beta)\propto\theta^{\alpha-1}(1-\theta)^{\beta-1}$。  

将贝塔分布和伯努利分布相乘，我们得到  

$$
{\begin{array}{r l}&{p(\theta\mid x,\alpha,\beta)\propto p(x\mid\theta)p(\theta\mid\alpha,\beta)}\\ &{\qquad\qquad\qquad\quad=\theta^{x}(1-\theta)^{1-x}\theta^{\alpha-1}(1-\theta)^{\beta-1}}\\ &{\qquad\qquad\quad=\theta^{\alpha+x-1}(1-\theta)^{\beta+(1-x)-1}}\\ &{\qquad\qquad\quad\propto p(\theta\mid\alpha+x,\beta+(1-x))\,.}\end{array}}
$$
最后一行是参数为 $(\alpha+x,\beta+(1-x))$ 的贝塔分布。  

表 6.2 列出了用于概率建模的一些标准似然函数参数的结合先验示例。例如，多项式、逆伽玛、逆威斯纳和狄利克雷分布可以在任何统计文本中找到，并在 Bishop (2006) 等中描述。  

贝塔分布是二项和伯努利似然函数中参数 $\mu$ 的结合先验。对于高斯似然函数，我们可以在均值上放置一个结合的高斯先验。表格中高斯似然函数出现两次的原因是我们需要区分单变量与多变量情况。在单变量（标量）情况下，逆伽玛是方差的结合先验。在多变量情况下，我们使用逆威斯纳分布作为协方差矩阵的先验。狄利克雷分布是多项式似然函数的结合先验。伽玛先验是单变量高斯似然函数中精度（逆方差）的结合先验，而威斯纳先验是多变量高斯似然函数中精度矩阵（逆协方差矩阵）的结合先验。  

对于进一步的细节，我们参考 Bishop (2006)。



### 6.6.2 充分统计量  

充分统计量 回忆一下，随机变量的统计量是一个随机变量的确定性函数。例如，如果 ${\pmb{x}}\,=\,[x_{1},\cdots,x_{N}]^{\top}$ 是一个一维高斯向量，即 $x_{n}\sim{\mathcal{N}}(\mu,\,\sigma^{2})$，那么样本均值 $\textstyle{\hat{\mu}}={\frac{1}{N}}(x_{1}+\cdot\cdot\cdot+x_{N})$ 是一个统计量。罗纳德·费希尔爵士发现了充分统计量的概念：即存在统计量，它们包含了从考虑的分布对应的数据中可以推断的所有可用信息。换句话说，充分统计量包含了进行关于总体的推断所需的所有信息，即它们是足以代表分布的统计量。

对于由 $\theta$ 参数化的分布集，让 $X$ 是一个随机变量，其分布为 $p(x\mid\theta_{0})$，给定未知的 $\theta_{0}$。一个统计量向量 $\phi(x)$ 被称为对于 $\theta_{0}$ 的充分统计量，如果它们包含了关于 $\theta_{0}$ 的所有可能信息。为了更正式地描述“包含了所有可能的信息”，这意味着给定 $\theta$ 的 $x$ 的概率可以分解为不依赖于 $\theta$ 的部分，以及仅通过 $\phi(x)$ 依赖于 $\theta$ 的部分。费希尔-奈曼分解定理形式化了这一概念，我们在定理 6.14 中不加证明地陈述如下。  

费希尔-奈曼定理




定理 6.14（费希尔-奈曼） . [定理 6.5 在 Lehmann 和 Casella (1998) 中]

设 $X$ 有概率密度函数 $p(x\mid\theta)$。则统计量

$\phi(x)$ 对于 $\theta$ 是充分统计量当且仅当 $p(x\mid\theta)$ 可以写成形式  

$$
p(x\,|\,\theta)=h(x)g_{\theta}(\phi(x))\,，
$$
其中 $h(x)$ 是与 $\theta$ 无关的分布，而 $g_{\theta}$ 通过充分统计量 $\phi(x)$ 捕捉了所有对 $\theta$ 的依赖性。

如果 $p(x\mid\theta)$ 不依赖于 $\theta$，那么 $\phi(x)$ 对于任何函数 $\phi$ 都是显而易见的充分统计量。更有趣的情况是 $p(x\mid\theta)$ 只依赖于 $\phi(x)$ 而不依赖于 $x$ 本身。在这种情况下，$\phi(x)$ 对于 $\theta$ 是充分统计量。

在机器学习中，我们考虑来自分布的有限数量的样本。可以想象，对于简单的分布（如示例 6.8 中的伯努利分布），我们只需要少量样本来估计分布的参数。我们也可以考虑相反的问题：如果我们有一组数据（未知分布的样本），哪个分布能给出最好的拟合？一个自然的问题是，随着我们观察到更多数据，描述分布是否需要更多的参数 $\theta$？通常情况下，答案是肯定的，这在非参数统计中进行了研究（Wasserman, 2007）。相反的问题是考虑哪些类的分布具有有限维的充分统计量，即描述它们所需的参数数量不会任意增加。答案是指数族分布，将在下节中描述。




### 6.6.3 指数族  

当我们考虑分布（离散或连续随机变量）时，可以有三种抽象层次。在第一层（光谱的最具体端），我们有一个特定命名的分布，参数固定，例如零均值和单位方差的一维高斯分布 $\mathcal{N}(0,\,1)$。在机器学习中，我们通常使用第二层抽象，即固定参数形式（一维高斯分布），并从数据中推断参数。例如，我们假设一维高斯分布 ${\mathcal{N}}({\boldsymbol{\mu}},\,{\boldsymbol{\sigma}}^{2})$，其中均值 $\mu$ 和方差 $\sigma^{2}$ 未知，并使用最大似然拟合来确定最佳参数 $(\mu,\sigma^{2})$。当我们考虑第 9 章中的线性回归时，会看到一个这样的例子。第三层抽象是考虑分布家族，而在本书中，我们考虑指数族。一维高斯分布是指数族成员的一个例子。包括表 6.2 中的所有“命名”模型在内的许多广泛使用的统计模型都是指数族的成员。它们都可以统一到一个概念中（Brown, 1986）。  

注释。 简短的历史轶事：像数学和科学中的许多概念一样，指数族是由不同研究人员在同一时间独立发现的。在 1935 年至 1936 年期间，塔斯马尼亚的埃德温·皮特曼、巴黎的乔治·达罗瓦和纽约的伯纳德·库普曼独立展示了指数族是唯一在重复独立抽样下享有有限维充分统计量的分布家族（Lehmann 和 Casella, 1998）。$\diamondsuit$  

指数族 是由参数 $\pmb{\theta}\in\mathbb{R}^{D}$ 特征化的概率分布家族，形式为  

$$
p(\pmb{x}\,|\,\pmb{\theta})=h(\pmb{x})\exp\left(\langle\pmb{\theta},\phi(\pmb{x})\rangle-A(\pmb{\theta})\right)\,，
$$
其中 $\phi({\pmb{x}})$ 是充分统计量的向量。一般来说，任何内积（第 3.2 节）都可以在（6.107）中使用，为了具体性，我们将在这里使用标准点积 $(\langle\pmb\theta,\phi(\pmb{x})\rangle=\pmb\theta^{\top}\phi(\pmb{x}))$。请注意，指数族本质上是费希尔-奈曼定理（定理 6.14）中 $g_{\theta}(\phi(x))$ 表达式的特定形式。  

因子 $h(x)$ 可以通过将对数 $h(\pmb{x})$ 添加到充分统计量向量 $\phi({\pmb{x}})$ 的另一个元素中，并约束相应的参数 $\theta_{0}\,=\,1$ 而被吸收到点积项中。术语 $A(\pmb\theta)$ 是归一化常数，确保分布总和或积分等于 1，并称为对数分区函数。通过忽略这两个术语并考虑指数族的形式  

$$
p(\pmb{x}\,|\,\pmb{\theta})\propto\exp\big(\pmb{\theta}^{\top}\phi(\pmb{x})\big)\,，
$$
可以得到指数族的直观概念。  

自然参数 对于这种参数化形式，参数 $\pmb{\theta}$ 被称为自然参数。乍一看，指数族似乎只是通过将指数函数添加到点积结果的平凡转换。然而，基于我们可以通过 $\phi({\pmb{x}})$ 捕获数据信息这一事实，有许多含义允许方便地建模和高效计算。



### 示例 6.13（高斯分布作为指数族）  

考虑一维高斯分布 ${\mathcal{N}}({\boldsymbol{\mu}},\,{\boldsymbol{\sigma}}^{2})$。令 $\phi(x)={\left[\begin{array}{l}{x}\\ {x^{2}}\end{array}\right]}$。通过使用指数族的定义，

$$
p(x\,|\,\theta)\propto\exp(\theta_{1}x+\theta_{2}x^{2})\,。
$$
设置  

$$
\theta=\left[{\frac{\mu}{\sigma^{2}}},-{\frac{1}{2\sigma^{2}}}\right]^{\top}
$$
并将 $\theta$ 代入（6.109），我们得到  

$$
p(x\,|\,\theta)\propto\exp\left(\frac{\mu x}{\sigma^{2}}-\frac{x^{2}}{2\sigma^{2}}\right)\propto\exp\left(-\frac{1}{2\sigma^{2}}(x-\mu)^{2}\right)\,。
$$
因此，一维高斯分布是指数族的成员，充分统计量为 $\phi(x)={\binom{x}{x^{2}}}$；自然参数由（6.110）中的 $\pmb{\theta}$ 给出。



### 示例 6.14（伯努利分布作为指数族）  

回想一下来自示例 6.8 的伯努利分布  

$$
p(x\,|\,\mu)=\mu^{x}(1-\mu)^{1-x}\,,\quad x\in\{0,1\}.
$$
这可以写成指数族形式  

$$
{\begin{array}{r l}&{p(x\,|\,\mu)=\exp\left[\log\left(\mu^{x}(1-\mu)^{1-x}\right)\right]}\\ &{\qquad\qquad=\exp\left[x\log\mu+(1-x)\log(1-\mu)\right]}\\ &{\qquad\qquad=\exp\left[x\log\mu-x\log(1-\mu)+\log(1-\mu)\right]}\\ &{\qquad\qquad=\exp\left[x\log{\frac{\mu}{1-\mu}}+\log(1-\mu)\right].}\end{array}}
$$
最后一行（6.113d）可以通过观察  

$$
h(x)=1
$$
$$
\begin{array}{c}{{\theta=\log\frac{\mu}{1-\mu}}}\\ {{\phi(x)=x}}\\ {{A(\theta)=-\log(1-\mu)=\log(1+\exp(\theta)).}}\end{array}
$$
来识别为指数族形式（6.107）。$\mu$ 与 $\theta$ 之间的关系称为对数函数或逻辑函数。注意 $\mu\ \in\ (0,1)$ 但 $\theta\ \in\ \mathbb{R}$，因此对数函数将实数值压缩到区间 $(0,1)$。这个特性在机器学习中很有用，例如在逻辑回归（Bishop, 2006, 第 4.3.2 节）中使用，以及在神经网络中的非线性激活函数（Goodfellow 等人，2016 年，第 6 章）。$\diamondsuit$  

通常不清楚如何找到特定分布的共轭分布的参数形式（例如，表 6.2 中的那些）。指数族提供了一种方便的方法来找到分布的共轭对。考虑随机变量 $X$ 是指数族（6.107）的成员：  

$$
p(\pmb{x}\,|\,\pmb{\theta})=h(\pmb{x})\exp\left(\langle\pmb{\theta},\phi(\pmb{x})\rangle-A(\pmb{\theta})\right)\,.
$$
指数族的每个成员都有一个共轭先验（Brown, 1986）  

$$
p(\pmb\theta\,|\,\gamma)=h_{c}(\pmb\theta)\exp\left(\left<\left[\gamma_{1}\right],\left[\begin{array}{c}{\pmb\theta}\\ {-A(\pmb\theta)}\end{array}\right]\right>-A_{c}(\gamma)\right)\,,
$$  

其中 $\gamma\,=\,\left[{\gamma_{1}\atop\gamma_{2}}\right]$ 的维度为 $\dim(\theta)+1$。共轭先验的充分统计量是 $\left[{\begin{array}{l}{\ \ \theta}\\ {-A(\theta)}\end{array}}\right]$。通过使用指数族共轭先验的一般形式的知识，我们可以推导出对应特定分布的共轭先验的函数形式。




### 示例 6.15  

回想一下伯努利分布的指数族形式（6.113d）  

$$
p(x\,|\,\mu)=\exp\left[x\log\frac{\mu}{1-\mu}+\log(1-\mu)\right].
$$  

共轭先验的对数形式为  

$$
p(\mu\,|\,\alpha,\beta)=\frac{\mu}{1-\mu}\exp\left[\alpha\log\frac{\mu}{1-\mu}+(\beta+\alpha)\log(1-\mu)-A_{c}(\gamma)\right]\,，
$$  

其中我们定义了 $\gamma\;:=\;[\alpha,\beta\,+\,\alpha]^{\top}$ 和 $h_{c}(\mu)\;:=\;\mu/(1\,-\,\mu)$。等式（6.122）简化为  

$$
p(\mu\,|\,\alpha,\beta)=\exp\left[(\alpha-1)\log\mu+(\beta-1)\log(1-\mu)-A_{c}(\alpha,\beta)\right]\,。
$$  

将此转换为非指数族形式得到  

$$
p(\boldsymbol{\mu}\,|\,\alpha,\beta)\propto\boldsymbol{\mu}^{\alpha-1}(1-\boldsymbol{\mu})^{\beta-1}\,，
$$  

我们将其识别为 Beta 分布（6.98）。在示例 6.12 中，我们假设 Beta 分布是伯努利分布的共轭先验，并证明了它确实是共轭先验。在这个示例中，我们通过观察伯努利分布的共轭先验在指数族形式下的形式，推导出了 Beta 分布的形式。  

如上一节所述，指数族的主要动机是它们具有有限维的充分统计量。此外，共轭分布很容易写出，而共轭分布也来自指数族。从推断的角度来看，最大似然估计行为良好，因为经验估计的充分统计量是充分统计量的最优估计（回想高斯的均值和协方差）。从优化的角度来看，对数似然函数是凹函数，允许应用高效的优化方法（第 7 章）。  

## 6.7 变量变换/逆变换  

可能看起来已知的分布非常多，但实际上我们有名字的分布集相当有限。因此，理解变换后随机变量的分布通常是很有用的。例如，假设随机变量 $X$ 按照一维正态分布 $\mathcal{N}(0,\,1)$ 分布，$X^{2}$ 的分布是什么？另一个例子，在机器学习中很常见，给定 $X_{1}$ 和 $X_{2}$ 是一维标准正态分布，$\textstyle{\frac{1}{2}}(X_{1}+X_{2})$ 的分布是什么？  

处理 $\textstyle{\frac{1}{2}}\big(X_{1}+X_{2}\big)$ 的分布的一个选项是计算 $X_{1}$ 和 $X_{2}$ 的均值和方差，然后将它们结合起来。正如我们在第 6.4.4 节中看到的，当我们考虑随机变量的仿射变换时，我们可以计算结果随机变量的均值和方差。然而，在变换下可能无法获得分布的函数形式。此外，我们可能对随机变量的非线性变换感兴趣，对于这些变换，闭式表达式可能不容易获得。  

注释（符号） . 本节中，我们将明确表示随机变量及其取值。因此，请记住我们使用大写字母 $X,Y$ 表示随机变量，小写字母 $x,y$ 表示目标空间 $\mathcal{T}$ 中随机变量的值。我们将离散随机变量 $X$ 的概率质量函数明确写为 $P(X=x)$。对于连续随机变量 $X$（第 6.2.2 节），概率密度函数写为 $f(x)$，累积分布函数写为 $F_{X}(x)$。$\diamondsuit$  

我们将探讨获得随机变量变换分布的两种方法：直接使用累积分布函数的定义的方法和使用微积分链式法则（第 5.2.2 节）的变量变换方法。变量变换方法广泛使用，因为它提供了一种尝试计算由于变换导致的结果分布的“食谱”。我们将解释单变量随机变量的方法，并仅简要提供多变量随机变量一般情况的结果。  

离散随机变量的变换可以直接理解。假设有一个离散随机变量 $X$，其概率质量函数为 $P(X=x)$（第 6.2.1 节），以及可逆函数 $U(x)$。考虑变换后的随机变量 $Y:=U(X)$，其概率质量函数为 $P(Y=y)$。然后  

矩生成函数也可以用于研究随机变量的变换（Casella 和 Berger, 2002, 第 2 章）。  

$$
\begin{array}{r c c}{P(Y=y)=P(U(X)=y)}&{\text{变换的逆}}\\ {=P(X=U^{-1}(y))}&{\text{逆变换}}\end{array}
$$  

我们可以观察到 $x\,=\,U^{-1}(y)$。因此，对于离散随机变量，变换直接改变个别事件（概率适当转换）。




### 6.7.1 分布函数技术  

分布函数技术基于第一原理，使用累积分布函数的定义 $F_{X}(x)=P(X\leqslant x)$ 和其微分是概率密度函数 $f(x)$ 的事实（Wasserman, 2004, 第 2 章）。对于随机变量 $X$ 和函数 $U$，我们通过以下步骤找到随机变量 $Y:=U(X)$ 的概率密度函数：

1. 找到累积分布函数：

$$
F_{Y}(y)=P(Y\leqslant y)
$$

2. 对累积分布函数 $F_{Y}(y)$ 进行微分以得到概率密度函数 $f(y)$。

$$
f(y)={\frac{\mathrm{d}}{\mathrm{d}y}}F_{Y}(y)\,。
$$
我们还需要记住，由于 $U$ 的变换，随机变量的域可能已经改变。

### 示例 6.16  

令 $X$ 为在 $0\leqslant x\leqslant1$ 上的概率密度函数为  

$$
f(x)=3x^{2}\,。
$$

我们感兴趣的是找到 $Y=X^{2}$ 的概率密度函数。

函数 $f$ 是 $x$ 的单调递增函数，因此结果值 $y$ 位于区间 $[0,1]$ 内。我们得到：

$$
{\begin{array}{r l r l}{F_{Y}(y)=P(Y\leqslant y)}&&{{\text{定义的累积分布函数}}}\\ {=P(X^{2}\leqslant y)}&&{{\text{变换的逆}}}\\ {=P(X\leqslant y^{\frac{1}{2}})}&&{{\text{逆变换}}}\\ {=F_{X}(y^{\frac{1}{2}})}&&{{\text{定义的累积分布函数}}}\\ {=\int_{0}^{y^{\frac{1}{2}}}3t^{2}\mathrm{d}t}&&{{\text{累积分布函数作为定义积分}}}\\ {=\left[t^{3}\right]_{t=0}^{t=y^{\frac{1}{2}}}}&&{{\text{积分结果}}}\\ {=y^{\frac{3}{2}},}&{0\leqslant y\leqslant1\,.}\end{array}}
$$

因此，$Y$ 的累积分布函数为  

$$
F_{Y}(y)=y^{\frac{3}{2}}
$$  

对于 $0\leqslant y\leqslant1$。为了获得概率密度函数，我们对累积分布函数进行微分：

$$
f(y)={\frac{\mathrm{d}}{\mathrm{d}y}}F_{Y}(y)={\frac{3}{2}}y^{\frac{1}{2}}
$$  

对于 $0\leqslant y\leqslant1$。

具有逆函数的函数称为双射函数（第 2.7 节）。

在示例 6.16 中，我们考虑了一个严格单调递增的函数 $f(x)=3x^{2}$。这意味着我们能够计算出逆函数。一般来说，我们要求感兴趣的函数 $y\,=\,U(x)$ 有一个逆函数 $x=U^{-1}(y)$。通过考虑随机变量 $X$ 的累积分布函数 $F_{X}(x)$ 并将其用作变换 $U(x)$，我们可以获得一个有用的结果。这导致以下定理。

定理 6.15. [Casella 和 Berger (2002) 中的定理 2.1.10] 让 $X$ 为一个连续随机变量，其累积分布函数严格单调递增为 $F_{X}(x)$。则定义为  

$$
Y:=F_{X}(X)
$$  

的随机变量 $Y$ 具有均匀分布。

定理 6.15 被称为概率积分变换，它用于通过将从均匀随机变量的采样结果转换为所需分布的采样来推导分布采样算法（Bishop, 2006）。算法首先生成均匀分布的样本，然后通过假设可用的逆累积分布函数对其进行转换，以获得所需分布的样本。概率积分变换还用于假设检验，以判断样本是否来自特定分布（Lehmann 和 Romano, 2005）。累积分布函数输出给定均匀分布的原理也构成了联合分布的基础（Nelsen, 2006）。




### 6.7.2 变量变换  

第 6.7.1 节中的分布函数技术基于第一原理，基于累积分布函数的定义和逆函数、微分和积分的性质。从第一原理出发的论证依赖于两个事实：

1. 我们可以将 $Y$ 的累积分布函数转换为 $X$ 的累积分布函数的表达式。

2. 我们可以对累积分布函数进行微分以获得概率密度函数。  

让我们逐步分解推理，以理解定理 6.16 中更一般的变量变换方法。  

注释。 “变量变换”的名称来源于在面对困难积分时改变积分变量的想法。对于单变量函数，我们使用积分的替换规则，  

$$
\int f(g(x))g^{\prime}(x)\mathrm{d}x=\int f(u)\mathrm{d}u\,,\quad\mathrm{where}\quad u=g(x)\,。
$$  

概率中的变量变换依赖于微积分中的变量变换方法（Tandra, 2014）。  

该规则的推导基于微积分中的链式法则（5.32）和应用微积分基本定理两次。微积分基本定理正式化了积分和微分在某种意义上是“逆操作”的事实。规则的直观理解可以通过思考（松散地）微小变化（微分）来获得等式 $u=g(x)$，即考虑 $\Delta u=g^{\prime}(x)\Delta x$ 作为 $u=g(x)$ 的微分。通过将 $u=g(x)$ 代入右侧积分的内部，等式（6.133）中的表达式变为 $f(g(x))$。通过假设 $\mathrm{d}u$ 可以近似为 $\mathrm{d}u\approx\Delta u=g^{\prime}(x)\Delta x,$，并且 $\mathrm{d}x\approx\Delta x$，我们得到（6.133）。♢  

考虑一个单变量随机变量 $X$，以及一个可逆函数 $U$，它给出了另一个随机变量 $Y=U(X)$。我们假设随机变量 $X$ 有状态 $x\,\in\,[a,b]$。根据累积分布函数的定义，我们有  

$$
F_{Y}(y)=P(Y\leqslant y)\,。
$$  

我们对函数 $U$ 的随机变量感兴趣  

$$
P(Y\leqslant y)=P(U(X)\leqslant y)\,，
$$  

我们假设函数 $U$ 是可逆的。在区间上的可逆函数要么严格增加要么严格减少。如果 $U$ 是严格增加的，则其逆函数 $U^{-1}$ 也是严格增加的。通过应用逆函数 $U^{-1}$ 到 $P(U(X)\leqslant y)$ 的参数，我们得到  

$$
P(U(X)\leqslant y)=P(U^{-1}(U(X))\leqslant U^{-1}(y))=P(X\leqslant U^{-1}(y))\,。
$$  

（6.136）中的最右项是 $X$ 的累积分布函数的表达式。回想一下，以概率密度函数的形式定义累积分布函数  

$$
P(X\leqslant U^{-1}(y))=\int_{a}^{U^{-1}(y)}f(x)\mathrm{d}x\,。
$$  

现在我们有 $Y$ 的累积分布函数的表达式以 $x$：  

$$
F_{Y}(y)=\int_{a}^{U^{-1}(y)}f(x)\mathrm{d}x\,。
$$  

为了获得概率密度函数，我们对（6.138）关于 $y$ 进行微分：  

$$
f(y)={\frac{\mathrm{d}}{\mathrm{d}y}}F_{y}(y)={\frac{\mathrm{d}}{\mathrm{d}y}}\int_{a}^{U^{-1}(y)}f(x)\mathrm{d}x\,。
$$  

注意右侧的积分是对 $x$ 进行的，但我们需要对 $y$ 进行积分，因为我们是关于 $y$ 进行微分。特别是，我们使用（6.133）来获取替换  

$$
\int f(U^{-1}(y)){U^{-1}}^{\prime}(y)\mathrm{d}y=\int f(x)\mathrm{d}x\quad\mathrm{where}\quad x=U^{-1}(y)\,。
$$  

使用（6.140）右侧的（6.139）给出  

$$
f(y)=\frac{\mathrm{d}}{\mathrm{d}y}\int_{a}^{U^{-1}(y)}f_{x}(U^{-1}(y))U^{-1^{\prime}}(y)\mathrm{d}y\,。
$$  

然后我们回忆起微分是一个线性运算符，并使用下标 $x$ 提醒自己 $f_{x}(U^{-1}(y))$ 是 $x$ 的函数而不是 $y$。再次应用微积分基本定理，我们得到  

$$
f(y)=f_{x}(U^{-1}(y))\cdot\left(\frac{\mathrm{d}}{\mathrm{d}y}U^{-1}(y)\right)\,。
$$  

我们假设 $U$ 是严格增加的函数。对于减少的函数，当我们遵循相同的推导时，实际上会出现负号。我们引入 $U$ 的绝对值的微分，以获得增加和减少 $U$ 的相同表达式：  

$$
f(y)=f_{x}(U^{-1}(y))\cdot\left|\frac{\mathrm{d}}{\mathrm{d}y}U^{-1}(y)\right|\,。
$$  

这被称为变量变换技术。表达式 $\begin{array}{r}{\left|\frac{\mathrm{d}}{\mathrm{d}y}U^{-1}(y)\right|}\end{array}$ 表示当应用 $U$ 时单位体积的变化（参见第 5.3 节中关于雅可比行列式的定义）。  

注释。 与离散情况（6.125b）相比，我们有额外的因子 $\begin{array}{r}{\left|\frac{\mathrm{d}}{\mathrm{d}y}U^{-1}(y)\right|}\end{array}$。连续情况需要更多的信息，因为对于多变量函数，绝对值无法使用。相反，我们使用雅可比矩阵的行列式。回想一下（5.58）中雅可比是偏导数的矩阵，非零行列式的存在表明我们可以逆雅可比。回想一下第 4.1 节中关于行列式出现的讨论，这是因为我们的微分（体积的立方）通过雅可比变换为平行六面体。让我们总结一下前面的讨论，给出以下定理，为我们提供了多变量变量变换的食谱。




定理 6.16. [Billingsley (1995) 中的定理 17.2] 让 $f(\pmb{x})$ 是多变量连续随机变量 $X$ 的概率密度的值。如果向量值函数 $\pmb{y}\,=\,U({\pmb{x}})$ 对于域内的所有 $x$ 值都是可微和可逆的，那么对于相应的 $\pmb{y}_{-}$ 值，$Y=U(X)$ 的概率密度由以下给出：

$$
f(\pmb{y})=f_{\pmb{x}}(U^{-1}(\pmb{y}))\cdot\left|\operatorname*{det}\left(\frac{\partial}{\partial\pmb{y}}U^{-1}(\pmb{y})\right)\right|.
$$

乍一看，定理看起来令人畏惧，但关键点是多变量随机变量的变量变换遵循单变量变量变换的程序。首先我们需要计算逆变换，并将结果代入 $x$ 的密度中。然后我们计算雅可比行列式的行列式并乘以结果。以下示例说明了双变量随机变量的情况。




### 示例 6.17  

考虑一个双变量随机变量 $X$，其状态为 $\pmb{x}=\left[\begin{array}{l}{x_{1}}\\ {x_{2}}\end{array}\right]$ 和概率密度函数  

$$
f\left({\binom{x_{1}}{x_{2}}}\right)={\frac{1}{2\pi}}\exp\left(-{\frac{1}{2}}\left[{x_{1}\atop x_{2}}\right]^{\top}\left[{x_{1}\atop x_{2}}\right]\right)\,。
$$  

我们使用定理 6.16 中的变量变换技术来推导线性变换（第 2.7 节）对随机变量的影响。考虑一个 $2\times2$ 的矩阵 $\pmb{A}\in\mathbb{R}^{2\times2}$，定义为  

$$
A=\left[\!\begin{array}{c c}a&b\\ c&d\end{array}\right]\,。
$$  

我们感兴趣的是找到状态为 $\pmb{y}=\pmb{A}\pmb{x}$ 的变换后的双变量随机变量 $Y$ 的概率密度函数。  

回想一下，对于变量变换，我们需要 $\pmb{x}$ 作为函数的逆变换 $\pmb{y}$。由于我们考虑线性变换，逆变换由矩阵逆给出（参见第 2.2.2 节）。对于 $2\times2$ 矩阵，我们可以明确写出公式，给出为  

$$
\left[{x_{1}\atop x_{2}}\right]={\pmb A}^{-1}\left[{y_{1}\atop y_{2}}\right]=\frac{1}{a d-b c}\left[\begin{array}{c c}{d}&{-b}\\ {-c}&{a}\end{array}\right]\left[\begin{array}{c c}{y_{1}}\\ {y_{2}}\end{array}\right].
$$  

注意 $a d-b c$ 是矩阵 $\pmb{A}$ 的行列式（第 4.1 节）。相应的概率密度函数给出为  

$$
\boldsymbol{f}(\boldsymbol{x})=\boldsymbol{f}(\boldsymbol{A}^{-1}\boldsymbol{y})=\frac{1}{2\pi}\exp\left(-\frac{1}{2}\boldsymbol{y}^{\intercal}\boldsymbol{A}^{-\intercal}\boldsymbol{A}^{-1}\boldsymbol{y}\right).
$$  

矩阵与向量的乘积对向量的偏导数是矩阵本身（第 5.5 节），因此  

$$
\frac{\partial}{\partial\pmb{y}}\pmb{A}^{-1}\pmb{y}=\pmb{A}^{-1}\,。
$$ 

从第 4.1 节中回忆，逆矩阵的行列式是行列式的倒数，因此雅可比矩阵的行列式是  

$$
\operatorname*{det}\left({\frac{\partial}{\partial y}}A^{-1}y\right)={\frac{1}{a d-b c}}\,。
$$  

现在我们可以通过将（6.148）与（6.150）相乘来应用定理 6.16 中的变量变换公式，这给出  

$$
\begin{array}{l}{{f(\pmb{y})=f(\pmb{x})\left|\mathrm{det}\left(\frac{\partial}{\partial\pmb{y}}\pmb{A}^{-1}\pmb{y}\right)\right|}}\\ {{\quad\quad=\displaystyle\frac{1}{2\pi}\exp\left(-\frac{1}{2}\pmb{y}^{\top}\pmb{A}^{-\top}\pmb{A}^{-1}\pmb{y}\right)\left|a d-b c\right|^{-1}.}}\end{array}
$$  

虽然示例 6.17 基于双变量随机变量，这使得我们能够轻松计算矩阵逆，但上述关系适用于更高维度。  

注释。 我们在第 6.5 节中看到，（6.148）中的密度 $f(\pmb{x})$ 实际上是标准高斯分布，而变换后的密度 $f(\pmb{y})$ 是协方差为 $\pmb{\Sigma}=\pmb{A}\pmb{A}^{\top}$ 的双变量高斯分布。 $\diamondsuit$  

我们将在这章中的想法应用于第 8.4 节中的概率建模，以及在第 8.5 节中引入图形语言。我们将看到这些想法在第 9 章和第 11 章中的直接机器学习应用。




## 6.8 进一步阅读  

本章有时较为简略。Grinstead 和 Snell (1997) 和 Walpole 等 (2011) 提供了更适合自我学习的更轻松的介绍。对概率的哲学方面感兴趣的读者可以考虑 Hacking (2001)，而与软件工程更相关的介绍则由 Downey (2014) 提供。关于指数族的概述可以在 Barndorff-Nielsen (2014) 中找到。在第 8 章中，我们将看到如何使用概率分布来建模机器学习任务。讽刺的是，神经网络兴趣的近期激增导致了对概率模型的更广泛认识。例如，正则化流动（Jimenez Rezende 和 Mohamed, 2015）依赖于变量变换来转换随机变量。关于变分推断方法在神经网络中的应用的概述可以在 Goodfellow 等 (2016) 书中的第 16 到 20 章中找到。  

我们通过避免测度论问题（Billingsley, 1995；Pollard, 2002）以及假设我们有实数以及定义实数上的集合及其适当出现频率的方法，来避免了连续随机变量的大部分困难。这些细节确实很重要，例如在连续随机变量 $x,y$ 的条件概率 $p(y\,|\,x)$ 的具体说明（Proschan 和 Presnell, 1998）。懒惰的符号隐藏了我们想要指定 $X\,=\,x$（这是一个测度为零的集合）的事实。此外，我们对 $y$ 的概率密度函数感兴趣。更精确的符号应该表示为 $\mathbb{E}_{y}[f(y)\,|\,\sigma(x)]$，其中我们对 $y$ 进行期望，条件于 $x$ 的 $\sigma$-代数。对概率理论细节感兴趣的更技术性读者有许多选择（Jaynes, 2003；MacKay, 2003；Jacod 和 Protter, 2004；Grimmett 和 Welsh, 2014），包括一些非常技术性的讨论（Shiryayev, 1984；Lehmann 和 Casella, 1998；Dudley, 2002；Bickel 和 Doksum, 2006；C ¸inlar, 2011）。概率的另一种方法是从期望的概念开始，并“向后工作”以推导概率空间所需的性质（Whittle, 2000）。随着机器学习允许我们对越来越复杂的数据类型建模更复杂的分布，概率机器学习模型的开发者需要理解这些更技术性的方面。具有概率建模重点的机器学习教材包括 MacKay (2003)；Bishop (2006)；Rasmussen 和 Williams (2006)；Barber (2012)；Murphy (2012)。

## 练习  

6.1 考虑两个离散随机变量 $X$ 和 $Y$ 的以下双变量分布 $p(x,y)$ 。  

计算：  

a. 边缘分布 $p(x)$ 和 $p(y)$ 。 b. 条件分布 $p(x|Y=y_{1})$ 和 $p(y|X=x_{3})$ 。  

6.2 考虑两个高斯分布的混合（如图 6.4 所示），  

$$
0.4\mathcal{N}\left(\left[\begin{array}{c}{10}\\ {2}\end{array}\right],\,\left[\begin{array}{c c}{1}&{0}\\ {0}&{1}\end{array}\right]\right)+0.6\mathcal{N}\left(\left[\begin{array}{c}{0}\\ {0}\end{array}\right],\,\left[\begin{array}{c c}{8.4}&{2.0}\\ {2.0}&{1.7}\end{array}\right]\right).
$$  

a. 计算每个维度的边缘分布。 b. 计算每个边缘分布的均值、模式和中位数。 c. 计算二维分布的均值和模式。  

6.3 你编写了一个计算机程序，有时可以编译，有时不能（代码不变）。你决定使用参数为 $\mu$ 的伯努利分布来建模编译器的表面随机性（成功 vs. 不成功） $x$ ：  

$$
p(x\,|\,\mu)=\mu^{x}(1-\mu)^{1-x}\,,\quad x\in\{0,1\}\,。
$$  

选择伯努利似然的共轭先验，并计算后验分布 $p(\mu\mid x_{1},\cdots,x_{N})$ 。  

6.4 有两个袋子。第一个袋子有四个芒果和两个苹果；第二个袋子有四个芒果和四个苹果。我们还有一个偏斜的硬币，显示“正面”的概率为 0.6，显示“反面”的概率为 0.4。如果硬币显示“正面”，我们随机从袋子 1 中挑选一个水果；否则我们随机从袋子 2 中挑选一个水果。你的朋友抛硬币（你无法看到结果），从相应的袋子中随机挑选一个水果，并向你展示一个芒果。从袋子 2 挑选芒果的概率是多少？提示：使用贝叶斯定理。




6.5 考虑时间序列模型  

$$
\begin{array}{r}{\pmb{x}_{t+1}=\pmb{A}\pmb{x}_{t}+\pmb{w}\,,\quad\pmb{w}\sim\mathcal{N}\big(\mathbf{0},\,\pmb{Q}\big)}\\ {\pmb{y}_{t}=\pmb{C}\pmb{x}_{t}+\pmb{v}\,,\quad\pmb{v}\sim\mathcal{N}\big(\mathbf{0},\,\pmb{R}\big)\,,}\end{array}
$$  

其中 $\pmb{w},\pmb{v}$ 是独立同分布的高斯噪声变量。进一步假设   $p(\pmb{x}_{0})=$ $\mathcal{N}\big(\pmb{\mu}_{0},\,\pmb{\Sigma}_{0}\big)$ 。  

 .  

a. $p(\pmb{x}_{0},\pmb{x}_{1},\cdots,\pmb{x}_{T})$ 的形式是什么？解释你的答案（你不必明确计算联合分布）。  

b. 假设   $p(\pmb{x}_{t}\,|\,\pmb{y}_{1},\cdots,\pmb{y}_{t})=\mathcal{N}\big(\pmb{\mu}_{t},\,\pmb{\Sigma}_{t}\big)$ 。 1. 计算 $p(\pmb{x}_{t+1}\,|\,\pmb{y}_{1},\cdots,\pmb{y}_{t})$ 。 2. 计算 $p(\pmb{x}_{t+1},\pmb{y}_{t+1}\,|\,\pmb{y}_{1},\dots,\pmb{y}_{t})$ 。 3. 在时间 $_{t+1}$ ，我们观察到值 $\pmb{y}_{t+1}=\hat{\pmb{y}}$ 。计算条件分布   $p(\pmb{x}_{t+1}\mid\pmb{y}_{1},\cdots,\pmb{y}_{t+1})$ 。  

6.6 证明（6.44）中的关系，该关系将标准定义的方差与原始分数表示的方差联系起来。  

6.7 证明（6.45）中的关系，该关系将数据集中示例之间的对齐与原始分数表示的方差联系起来。

6.8 将伯努利分布表示为指数家族的自然参数形式，参见（6.107）。  

6.9 将二项分布表示为指数家族分布。同时表示贝塔分布为指数家族分布。展示贝塔分布和二项分布的乘积也是指数家族的成员。  

6.10 以两种方式推导第 6.5.2 节中的关系：  

a. 完成平方 b. 通过将高斯表示为其指数家族形式 高斯分布 产品 两个 G $c\mathcal{N}(\pmb{x}\,|\,\pmb{c},\,\pmb{C})$  $\mathcal{N}\big(\pmb{x}\,|\,\pmb{a},\,\pmb{A}\big)\mathcal{N}\big(\pmb{x}\,|\,\pmb{b},\,\pmb{B}\big)$    其中  |  N   |    是未归一化的 $\begin{array}{r l}&{C=(A^{-1}+B^{-1})^{-1}}\\ &{c=C(A^{-1}a+B^{-1}b)}\\ &{c=(2\pi)^{-\frac{D}{2}}\left|\,A+B\,\right|^{-\frac{1}{2}}\exp\big(-\frac{1}{2}(a-b)^{\top}(A+B)^{-1}(a-b)\big)\,.}\end{array}$ 注意归一化常数   $c$    它本身可以被视为 $\pmb{a}$ 或在   $\pmb{b}$ 中的（归一化）高斯分布，具有“膨胀”的协方差矩阵 $A+B$ ，即 $c=\mathcal{N}\big(a\,|\,b,\,A+B\big)=\mathcal{N}\big(b\,|\,a,\,A+B\big)$ 。




6.11 递归期望  

考虑两个随机变量 $x,y$ 以及它们的联合分布 $p(x,y)$ 。证明  

$$
\mathbb{E}_{X}[x]=\mathbb{E}_{Y}\big[\mathbb{E}_{X}[x\,|\,y]\big]\,。
$$ 

这里，$\mathbb{E}_{X}[x\,|\,y]$ 表示在条件分布 $p(x\,|\,y)$ 下 $x$ 的期望值。  

6.12 高斯随机变量的操作  

考虑一个高斯随机变量 $\mathbf{\mathcal{X}}\sim{\mathcal{N}}{\left(\mathbf{\mathcal{X}}\mid\mu_{x},\,\Sigma_{x}\right)}$ ，其中 $\pmb{x}\in\mathbb{R}^{D}$ 。此外，我们有  

$$
\pmb{y}=\pmb{A}\pmb{x}+\pmb{b}+\pmb{w}\,，
$$  

其中 $\pmb{y}\,\in\,\mathbb{R}^{E},\,\pmb{A}\,\in\,\mathbb{R}^{E\times D},\,\pmb{b}\,\in\,\mathbb{R}^{E}$ ，且   ${\pmb w}\,\sim\mathcal{N}\big({\pmb w}\,|\,{\bf0},\,{\pmb Q}\big)$ 是独立的高斯噪声。“独立”意味着 $x$ 和 $w$ 是独立的随机变量，且 $\boldsymbol{Q}$ 是对角矩阵。  

a. 写下似然函数 $p(\pmb{y}\mid\pmb{x})$ 。 b. 分布   $\begin{array}{r}{p(\pmb{y})=\int p(\pmb{y}\,|\,\pmb{x})p(\pmb{x})d\pmb{x}}\end{array}$ 是高斯分布。计算均值 $\pmb{\mu}_{y}$ 和协方差 $\Sigma_{y}$ 。详细推导你的结果。  

  

c. 随机变量   $\pmb{y}$ 根据测量映射被转换为  

$$
\boldsymbol{z}=C\boldsymbol{y}+\boldsymbol{v}\,，
$$

其中   $z\in\mathbb{R}^{F}$ ，   $\boldsymbol{C}\in\mathbb{R}^{F\times E}$ ，且   ${\pmb v}\sim\mathcal{N}({\pmb v}\mid{\bf0},\,R)$ 是独立的高斯（测量）噪声。  

写出 $p(z\mid\pmb{y})$ 。计算   $p(z)$ ，即均值   $\pmb{\mu}_{z}$ 和协方差   $\Sigma_{z}$ 。详细推导你的结果。 d. 现在测量了一个值 $\hat{\pmb{y}}$ 。计算后验分布   $p(\pmb{x}\,|\,\hat{\pmb{y}})$ 。解决方案提示：这个后验也是高斯分布，即我们只需要确定它的均值和协方差矩阵。首先明确计算联合高斯分布 $p(\pmb{x},\pmb{y})$ 。这也需要我们计算交叉协方差 $\operatorname{Cov}_{\pmb{x},\pmb{y}}[\pmb{x},\pmb{y}]$ 和 $\operatorname{Cov}_{\pmb{y},\pmb{x}}[\pmb{y},\pmb{x}]$ 。然后应用高斯条件的规则。

6.13 概率积分变换  

给定一个连续随机变量 $X$ ，其累积分布函数为 $F_{X}(x)$ ，证明随机变量   $Y:=F_{X}(X)$ 是均匀分布的（定理 6.15）。