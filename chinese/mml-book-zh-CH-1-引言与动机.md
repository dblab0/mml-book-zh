
# 第一部分

# 数学基础

# 第一章  引言与动机  

机器学习是关于设计能够自动从数据中提取有价值信息的算法。这里的重点在于“自动”，即机器学习关注的是可以应用于大量数据集的一般性方法，同时产生有意义的结果。机器学习的核心概念有三个：数据、模型和学习。

由于机器学习本质上是数据驱动的，数据是机器学习的核心。机器学习的目标是设计一般性的方法来从数据中提取有价值的模式，理想情况下，无需太多特定领域的专业知识。例如，给定一个大型文档集合（例如，图书馆中的大量书籍），可以使用机器学习方法自动找到贯穿文档的关联主题（Hoffman等人，2010）。为了实现这一目标，我们设计模型，通常与生成数据的过程相关，类似于我们所给的数据集。例如，在回归设置中，模型会描述一个函数，该函数将输入映射到实数值输出。引用Mitchell（1997）的话：如果模型在考虑数据后在给定任务上的表现有所提高，那么可以说模型从数据中学习。目标是找到能够很好地泛化到未见过的数据的优秀模型，我们可能在未来关心这些数据。

尽管机器学习已经取得了许多成功案例，且现成的软件可用于设计和训练丰富且灵活的机器学习系统，但我们认为理解机器学习的数学基础对于理解构建更复杂机器学习系统的基本原理至关重要。理解这些原理有助于创建新的机器学习解决方案，理解现有方法的优缺点，以及了解我们正在使用的方法的内在假设和限制。

## 1.1 为直觉寻找词汇  

预测器  

训练  

在机器学习中，我们经常面临的一个挑战是概念和词汇的模糊性，机器学习系统中的特定组件可以被抽象为不同的数学概念。例如，“算法”这个词在机器学习的上下文中至少有两种不同的含义。在第一种意义上，我们使用“机器学习算法”这一短语来指代基于输入数据进行预测的系统。我们将这些算法称为**预测器**。在第二种意义上，我们使用完全相同的短语“机器学习算法”来指代一个系统，该系统调整预测器的内部参数，使其在未来的未见输入数据上表现良好。在这里，我们将这种调整称为**训练**系统。

本书不会解决歧义问题，但我们希望明确指出，根据上下文，相同的表达可能意味着不同的事物。然而，我们尝试使上下文足够清晰，以减少歧义的程度。

本书的第一部分介绍了讨论机器学习系统三大组成部分所需的基本数学概念和基础：数据、模型和学习。我们现在简要概述这些组成部分，并在第8章讨论必要的数学概念后，再次回顾它们。

并非所有数据都是数值型的，但将其视为数值格式通常是有用的。在本书中，我们假设数据已经适当转换为适合计算机程序读取的数值表示形式。因此，我们将数据视为向量。关于向量的思考方式至少有三种：作为一组数字的数组（计算机科学视角）、具有方向和大小的箭头（物理学视角）以及遵循加法和缩放的物体（数学视角）。

**模型**通常用于描述生成数据的过程，类似于手头的数据集。因此，好的模型也可以被视为真实（未知）数据生成过程的简化版本，捕捉到与数据建模和从中提取隐藏模式相关的关键方面。一个好的模型然后可以用于预测在真实世界中会发生什么，而无需进行现实世界实验。

现在，我们来到机器学习的核心问题，即**学习**部分。假设我们被给予一个数据集和一个合适的模型。**训练**模型意味着使用可用的数据来优化模型的一些参数，以针对评估模型预测训练数据性能的效用函数。大多数训练方法可以被视为类似于攀登山峰以达到其顶点的途径的类比。在这个类比中，山峰对应于某些期望性能度量的最大值。然而，在实践中，我们对模型在未见数据上表现良好感兴趣。在已经看到的数据（训练数据）上表现良好可能仅意味着我们找到了一种很好的方式来记住数据。然而，这可能不适用于未见数据，而在实际应用中，我们通常需要让机器学习系统接触到它未曾遇到过的情况。

让我们总结一下本书中涵盖的主要机器学习概念：

我们用向量表示数据。我们选择合适的模型，使用概率论或优化的观点。通过使用数值优化方法从可用数据中学习，我们的目标是使模型在未用于训练的数据上表现良好。


## 1.2 本书的两种阅读策略  

我们可以考虑两种理解机器学习数学的方法策略：

自下而上：从基础概念构建到更高级的概念。这种方法在更技术性的领域，如数学，通常更受欢迎。这种方法的优点在于读者始终可以依赖他们之前学习的概念。不幸的是，对于实践者来说，许多基础概念本身并不特别有趣，缺乏动机意味着大多数基础定义很快就会被遗忘。

自上而下：从实际需求深入到更基本的要求。这种目标驱动的方法的优点在于读者始终知道他们需要专注于特定概念的原因，并且有一个明确的知识路径。这种方法的缺点是知识建立在可能不稳固的基础上，读者需要记住一组他们无法理解的词汇。

我们决定以模块化的方式编写本书，将基础（数学）概念与应用分开，以便本书可以以两种方式阅读。本书分为两部分，其中第一部分建立数学基础，第二部分将第一部分的概念应用于一组基本的机器学习问题，这些问题形成了机器学习的四大支柱，如图1.1所示：回归、降维、密度估计和分类。第一部分的章节大多建立在之前的章节上，但如果必要，可以跳过一章并向前回溯。第二部分的章节之间联系较松，可以按任意顺序阅读。两部分之间有许多前后指针，

![](images/109f5c161dd9a000c3b3cfc2e2e4e79892012fc6daaf30ac0cfb3f2aa5322870.jpg)  
图1.1 机器学习的基础和四大支柱。  

将数学概念与机器学习算法联系起来。

当然，阅读本书的方法不止两种。大多数读者使用自上而下和自下而上的结合策略学习，有时在尝试更复杂的概念之前先建立基本的数学技能，但也会根据机器学习的应用选择主题。


# 第一部分 关于数学  

本书中讨论的机器学习的四大支柱（见图1.1）需要坚实的数学基础，这部分基础在第一部分中建立。

线性代数  

解析几何  

矩阵分解  

我们将数值数据表示为向量，并将这样的数据表表示为矩阵。向量和矩阵的研究称为线性代数，这部分在第2章中介绍。向量作为矩阵的集合也在那里描述。

给定两个表示现实世界中两个对象的向量，我们希望对它们的相似性做出陈述。想法是相似的向量应该由我们的机器学习算法（我们的预测器）预测出相似的输出。为了正式化向量之间的相似性概念，我们需要引入将两个向量作为输入并返回表示它们相似性的数值的运算。相似性和距离的构建是解析几何的核心，并在第3章中讨论。

在第4章中，我们引入了关于矩阵和矩阵分解的一些基本概念。矩阵上的某些操作在机器学习中非常有用，它们允许我们直观地解释数据，并更有效地学习。

我们通常将数据视为某些真实基础信号的噪声观察。我们希望通过应用机器学习来识别信号。这需要我们有一个衡量“噪声”意味着什么的语言。我们通常也希望有允许我们表达某种不确定性的预测器，例如，量化我们在特定测试数据点对预测值的信心。不确定性量化的领域是概率论，概率论在第6章中讨论。

为了训练机器学习模型，我们通常找到最大化某些性能指标的参数。许多优化技术需要梯度的概念，它告诉我们寻找解决方案的方向。第5章是关于向量微积分，详细介绍了向量微积分中的梯度概念，我们在第7章中讨论了优化，用于找到函数的最大值/最小值。


# 第二部分 关于机器学习  

本书的第二部分介绍了图1.1中显示的机器学习的四大支柱。我们将展示第一部分书中介绍的数学概念是如何为每个支柱奠定基础的。大致而言，章节按照难度排序（从低到高）。

在第8章中，我们将机器学习的三个组成部分（数据、模型和参数估计）以数学方式重新表述，并提供了一些构建实验设置的指南，以防止对机器学习系统进行过于乐观的评估。记住我们的目标是构建一个在未见过的数据上表现良好的预测器。

在第9章中，我们将深入探讨线性回归，我们的目标是找到将输入$\pmb{x}\in\mathbb{R}^{D}$ 映射到对应观察到的功能值$y\in\mathbb{R}$ 的函数，我们可以将这些解释为它们各自输入的标签。我们将讨论经典模型拟合（参数估计）通过最大似然估计和最大后验估计，以及贝叶斯线性回归，其中我们通过整合参数而不是优化它们来实现。

第10章专注于降维，图1.1中的第二支柱，使用主成分分析。降维的关键目标是找到高维数据$\pmb{x}\,\in\,\mathbb{R}^{D}$ 的紧凑、低维表示，这通常比原始数据更容易分析。与回归不同，降维只关心建模数据——数据点$\pmb{x}$ 没有标签。

在第11章中，我们将转向第三支柱：密度估计。密度估计的目标是找到描述给定数据集的概率分布。我们将专注于高斯混合模型，讨论寻找该模型参数的迭代方案。与降维类似，数据点$\pmb{x}\in\mathbb{R}^{D}$ 没有标签。然而，我们不寻求数据的低维表示。相反，我们对描述数据的密度模型感兴趣。

第12章以深入讨论第四支柱：分类结束本书。我们将讨论分类的上下文中的支持向量机。类似于第9章的回归，我们有输入$x$ 和对应的标签$y$ 。然而，与回归不同，其中的标签是整数，这需要特别注意。

## 1.3 练习与反馈

在第一部分中，我们提供了一些可以通过笔和纸完成的练习。对于第二部分，我们提供了编程教程（jupyter笔记本），以探索本书中讨论的机器学习算法的一些特性。

我们感谢剑桥大学出版社强烈支持我们的目标，即通过使本书免费下载，实现教育和学习的民主化。您可以在以下网址找到教程、勘误表和额外材料：

如果您发现错误或有反馈，可以通过上述网址报告并提供反馈。



